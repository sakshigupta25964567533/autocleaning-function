{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6dbd3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezdxf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import LineString, MultiLineString\n",
    "from shapely.ops import linemerge\n",
    "import networkx as nx\n",
    "from sklearn.cluster import DBSCAN\n",
    "from difflib import get_close_matches\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import ezdxf\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any\n",
    "import ezdxf\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import List, Tuple, Set, Dict, Any, Optional, Union\n",
    "import logging\n",
    "from collections import defaultdict, deque\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829893f",
   "metadata": {},
   "source": [
    "# overlapping lines and duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac31bc5b-3200-491f-99cb-8712666ea5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger('ezdxf-overkill')\n",
    "\n",
    "class Vector:\n",
    "    def __init__(self, x: float, y: float, z: float = 0.0):\n",
    "        self.x = float(x)\n",
    "        self.y = float(y)\n",
    "        self.z = float(z)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_points(cls, p1: Tuple[float, float, float], p2: Tuple[float, float, float]):\n",
    "        return cls(p2[0] - p1[0], p2[1] - p1[1], p2[2] - p1[2])\n",
    "    \n",
    "    def dot(self, other):\n",
    "        return self.x * other.x + self.y * other.y + self.z * other.z\n",
    "    \n",
    "    def cross(self, other):\n",
    "        return Vector(\n",
    "            self.y * other.z - self.z * other.y,\n",
    "            self.z * other.x - self.x * other.z,\n",
    "            self.x * other.y - self.y * other.x\n",
    "        )\n",
    "    \n",
    "    def length(self):\n",
    "        return math.sqrt(self.x**2 + self.y**2 + self.z**2)\n",
    "    \n",
    "    def normalize(self):\n",
    "        length = self.length()\n",
    "        if length == 0:\n",
    "            return Vector(0, 0, 0)\n",
    "        return Vector(self.x/length, self.y/length, self.z/length)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Vector({self.x}, {self.y}, {self.z})\"\n",
    "\n",
    "class BoundingBox:\n",
    "    def __init__(self, min_x, min_y, min_z, max_x, max_y, max_z):\n",
    "        self.min_x = min_x\n",
    "        self.min_y = min_y\n",
    "        self.min_z = min_z\n",
    "        self.max_x = max_x\n",
    "        self.max_y = max_y\n",
    "        self.max_z = max_z\n",
    "    \n",
    "    @classmethod\n",
    "    def from_points(cls, points):\n",
    "        if not points:\n",
    "            return cls(0, 0, 0, 0, 0, 0)\n",
    "        \n",
    "        min_x = min_y = min_z = float('inf')\n",
    "        max_x = max_y = max_z = float('-inf')\n",
    "        \n",
    "        for point in points:\n",
    "            min_x = min(min_x, point[0])\n",
    "            min_y = min(min_y, point[1])\n",
    "            min_z = min(min_z, point[2] if len(point) > 2 else 0)\n",
    "            max_x = max(max_x, point[0])\n",
    "            max_y = max(max_y, point[1])\n",
    "            max_z = max(max_z, point[2] if len(point) > 2 else 0)\n",
    "        \n",
    "        return cls(min_x, min_y, min_z, max_x, max_y, max_z)\n",
    "    \n",
    "    def overlaps(self, other, tolerance=0):\n",
    "        return (\n",
    "            self.max_x + tolerance >= other.min_x and\n",
    "            self.min_x - tolerance <= other.max_x and\n",
    "            self.max_y + tolerance >= other.min_y and\n",
    "            self.min_y - tolerance <= other.max_y and\n",
    "            self.max_z + tolerance >= other.min_z and\n",
    "            self.min_z - tolerance <= other.max_z\n",
    "        )\n",
    "    \n",
    "    def distance_to(self, other):\n",
    "        dx = max(0, max(self.min_x - other.max_x, other.min_x - self.max_x))\n",
    "        dy = max(0, max(self.min_y - other.max_y, other.min_y - self.max_y))\n",
    "        dz = max(0, max(self.min_z - other.max_z, other.min_z - self.max_z))\n",
    "        \n",
    "        return math.sqrt(dx*dx + dy*dy + dz*dz)\n",
    "\n",
    "class GridIndex:\n",
    "    def __init__(self, cell_size=1.0):\n",
    "        self.cell_size = cell_size\n",
    "        self.grid = defaultdict(list)\n",
    "        self.entity_cells = {}\n",
    "    \n",
    "    def _get_cell_coords(self, point):\n",
    "        x, y, z = point\n",
    "        return (int(x / self.cell_size), int(y / self.cell_size), int(z / self.cell_size))\n",
    "    \n",
    "    def _get_cells_for_bbox(self, bbox):\n",
    "        min_cell_x = int(bbox.min_x / self.cell_size)\n",
    "        min_cell_y = int(bbox.min_y / self.cell_size)\n",
    "        min_cell_z = int(bbox.min_z / self.cell_size)\n",
    "        max_cell_x = int(bbox.max_x / self.cell_size) + 1\n",
    "        max_cell_y = int(bbox.max_y / self.cell_size) + 1\n",
    "        max_cell_z = int(bbox.max_z / self.cell_size) + 1\n",
    "        \n",
    "        cells = []\n",
    "        for x in range(min_cell_x, max_cell_x):\n",
    "            for y in range(min_cell_y, max_cell_y):\n",
    "                for z in range(min_cell_z, max_cell_z):\n",
    "                    cells.append((x, y, z))\n",
    "        \n",
    "        return cells\n",
    "    \n",
    "    def insert(self, entity, bbox):\n",
    "        handle = get_entity_handle(entity)\n",
    "        cells = self._get_cells_for_bbox(bbox)\n",
    "        \n",
    "        for cell in cells:\n",
    "            self.grid[cell].append(entity)\n",
    "        \n",
    "        self.entity_cells[handle] = cells\n",
    "    \n",
    "    def remove(self, entity):\n",
    "        handle = get_entity_handle(entity)\n",
    "        \n",
    "        if handle in self.entity_cells:\n",
    "            for cell in self.entity_cells[handle]:\n",
    "                if cell in self.grid and entity in self.grid[cell]:\n",
    "                    self.grid[cell].remove(entity)\n",
    "            \n",
    "            del self.entity_cells[handle]\n",
    "    \n",
    "    def query(self, bbox):\n",
    "        cells = self._get_cells_for_bbox(bbox)\n",
    "        \n",
    "        result = set()\n",
    "        for cell in cells:\n",
    "            result.update(self.grid.get(cell, []))\n",
    "        \n",
    "        return list(result)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.grid.clear()\n",
    "        self.entity_cells.clear()\n",
    "\n",
    "def get_entity_handle(entity):\n",
    "    \"\"\"Safely get an entity's handle with fallbacks.\"\"\"\n",
    "    if hasattr(entity, 'dxf') and hasattr(entity.dxf, 'handle'):\n",
    "        return entity.dxf.handle\n",
    "    elif hasattr(entity, 'handle'):\n",
    "        return entity.handle\n",
    "    else:\n",
    "        # Fallback to using object ID if no handle is available\n",
    "        return id(entity)\n",
    "\n",
    "def point_distance(p1: Tuple[float, float, float], p2: Tuple[float, float, float]) -> float:\n",
    "    return math.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2 + (p2[2] - p1[2])**2)\n",
    "\n",
    "def is_same_point(p1: Tuple[float, float, float], p2: Tuple[float, float, float], tolerance: float) -> bool:\n",
    "    return point_distance(p1, p2) <= tolerance\n",
    "\n",
    "def point_to_line_distance(point: Tuple[float, float, float], \n",
    "                          line_start: Tuple[float, float, float], \n",
    "                          line_end: Tuple[float, float, float]) -> float:\n",
    "    if line_start == line_end:\n",
    "        return point_distance(point, line_start)\n",
    "    \n",
    "    line_vec = Vector.from_points(line_start, line_end)\n",
    "    point_vec = Vector.from_points(line_start, point)\n",
    "    \n",
    "    line_length = line_vec.length()\n",
    "    if line_length == 0:\n",
    "        return point_vec.length()\n",
    "    \n",
    "    line_vec_normalized = line_vec.normalize()\n",
    "    \n",
    "    dot_product = point_vec.dot(line_vec_normalized)\n",
    "    \n",
    "    proj_vec = Vector(\n",
    "        line_vec_normalized.x * dot_product,\n",
    "        line_vec_normalized.y * dot_product,\n",
    "        line_vec_normalized.z * dot_product\n",
    "    )\n",
    "    \n",
    "    perp_vec = Vector(\n",
    "        point_vec.x - proj_vec.x,\n",
    "        point_vec.y - proj_vec.y,\n",
    "        point_vec.z - proj_vec.z\n",
    "    )\n",
    "    \n",
    "    return perp_vec.length()\n",
    "\n",
    "def point_on_line(point: Tuple[float, float, float], \n",
    "                 line_start: Tuple[float, float, float], \n",
    "                 line_end: Tuple[float, float, float], \n",
    "                 tolerance: float) -> bool:\n",
    "    distance = point_to_line_distance(point, line_start, line_end)\n",
    "    \n",
    "    if distance > tolerance:\n",
    "        return False\n",
    "    \n",
    "    line_length = point_distance(line_start, line_end)\n",
    "    p_to_start = point_distance(point, line_start)\n",
    "    p_to_end = point_distance(point, line_end)\n",
    "    \n",
    "    return p_to_start <= line_length + tolerance and p_to_end <= line_length + tolerance\n",
    "\n",
    "def are_collinear(p1: Tuple[float, float, float], p2: Tuple[float, float, float], \n",
    "                 p3: Tuple[float, float, float], tolerance: float) -> bool:\n",
    "    v1 = Vector.from_points(p1, p2)\n",
    "    v2 = Vector.from_points(p1, p3)\n",
    "    \n",
    "    cross = v1.cross(v2)\n",
    "    \n",
    "    return cross.length() <= tolerance * max(v1.length(), v2.length())\n",
    "\n",
    "def get_closest_point_on_line(point: Tuple[float, float, float], \n",
    "                             line_start: Tuple[float, float, float], \n",
    "                             line_end: Tuple[float, float, float]) -> Tuple[float, float, float]:\n",
    "    if line_start == line_end:\n",
    "        return line_start\n",
    "    \n",
    "    line_vec = Vector.from_points(line_start, line_end)\n",
    "    point_vec = Vector.from_points(line_start, point)\n",
    "    \n",
    "    line_length_sq = line_vec.dot(line_vec)\n",
    "    if line_length_sq == 0:\n",
    "        return line_start\n",
    "    \n",
    "    dot_product = point_vec.dot(line_vec) / line_length_sq\n",
    "    \n",
    "    dot_product = max(0, min(1, dot_product))\n",
    "    \n",
    "    return (\n",
    "        line_start[0] + dot_product * (line_end[0] - line_start[0]),\n",
    "        line_start[1] + dot_product * (line_end[1] - line_start[1]),\n",
    "        line_start[2] + dot_product * (line_end[2] - line_start[2])\n",
    "    )\n",
    "\n",
    "def are_lines_colinear(line1_start: Tuple[float, float, float], line1_end: Tuple[float, float, float],\n",
    "                       line2_start: Tuple[float, float, float], line2_end: Tuple[float, float, float],\n",
    "                       tolerance: float) -> bool:\n",
    "    if not are_collinear(line1_start, line1_end, line2_start, tolerance):\n",
    "        return False\n",
    "    if not are_collinear(line1_start, line1_end, line2_end, tolerance):\n",
    "        return False\n",
    "    \n",
    "    v1 = Vector.from_points(line1_start, line1_end)\n",
    "    v2 = Vector.from_points(line2_start, line2_end)\n",
    "    \n",
    "    if v1.length() == 0 or v2.length() == 0:\n",
    "        return True\n",
    "    \n",
    "    v1 = v1.normalize()\n",
    "    v2 = v2.normalize()\n",
    "    \n",
    "    dot_product = abs(v1.dot(v2))\n",
    "    \n",
    "    return abs(dot_product - 1) <= tolerance\n",
    "\n",
    "def lines_overlap(line1_start: Tuple[float, float, float], line1_end: Tuple[float, float, float],\n",
    "                 line2_start: Tuple[float, float, float], line2_end: Tuple[float, float, float],\n",
    "                 tolerance: float) -> Tuple[bool, Optional[Tuple[Tuple[float, float, float], Tuple[float, float, float]]]]:\n",
    "    if not are_lines_colinear(line1_start, line1_end, line2_start, line2_end, tolerance):\n",
    "        return (False, None)\n",
    "    \n",
    "    v1 = Vector.from_points(line1_start, line1_end)\n",
    "    \n",
    "    if v1.length() == 0:\n",
    "        if is_same_point(line1_start, line2_start, tolerance) or is_same_point(line1_start, line2_end, tolerance):\n",
    "            return (True, (line1_start, line1_start))\n",
    "        return (False, None)\n",
    "    \n",
    "    v1 = v1.normalize()\n",
    "    \n",
    "    def project_point(point):\n",
    "        v = Vector.from_points(line1_start, point)\n",
    "        return v.dot(v1)\n",
    "    \n",
    "    t1_start = 0.0\n",
    "    t1_end = project_point(line1_end)\n",
    "    t2_start = project_point(line2_start)\n",
    "    t2_end = project_point(line2_end)\n",
    "    \n",
    "    if t1_end < t1_start:\n",
    "        t1_start, t1_end = t1_end, t1_start\n",
    "    if t2_end < t2_start:\n",
    "        t2_start, t2_end = t2_end, t2_start\n",
    "    \n",
    "    if t1_end < t2_start - tolerance or t2_end < t1_start - tolerance:\n",
    "        return (False, None)\n",
    "    \n",
    "    overlap_start = max(t1_start, t2_start)\n",
    "    overlap_end = min(t1_end, t2_end)\n",
    "    \n",
    "    def get_point_at_param(t):\n",
    "        return (\n",
    "            line1_start[0] + t * v1.x,\n",
    "            line1_start[1] + t * v1.y,\n",
    "            line1_start[2] + t * v1.z\n",
    "        )\n",
    "    \n",
    "    p_start = get_point_at_param(overlap_start)\n",
    "    p_end = get_point_at_param(overlap_end)\n",
    "    \n",
    "    return (True, (p_start, p_end))\n",
    "\n",
    "def get_entity_dxf_attribs(entity) -> Dict[str, Any]:\n",
    "    attribs = {}\n",
    "    common_attribs = [\n",
    "        'layer', 'linetype', 'color', 'lineweight', 'ltscale', \n",
    "        'invisible', 'true_color', 'transparency'\n",
    "    ]\n",
    "    \n",
    "    if hasattr(entity, 'dxf'):\n",
    "        for attr in common_attribs:\n",
    "            if hasattr(entity.dxf, attr):\n",
    "                try:\n",
    "                    value = getattr(entity.dxf, attr)\n",
    "                    if value is not None:\n",
    "                        attribs[attr] = value\n",
    "                except (AttributeError, ValueError):\n",
    "                    pass\n",
    "    \n",
    "    return attribs\n",
    "\n",
    "def get_line_coords(line) -> Tuple[Tuple[float, float, float], Tuple[float, float, float]]:\n",
    "    # Check if we're dealing with an ezdxf entity or custom data structure\n",
    "    if hasattr(line, 'dxf'):\n",
    "        if hasattr(line.dxf, 'start'):\n",
    "            if hasattr(line.dxf.start, 'z'):\n",
    "                start = (line.dxf.start.x, line.dxf.start.y, line.dxf.start.z)\n",
    "            else:\n",
    "                start = (line.dxf.start.x, line.dxf.start.y, 0.0)\n",
    "            \n",
    "            if hasattr(line.dxf.end, 'z'):\n",
    "                end = (line.dxf.end.x, line.dxf.end.y, line.dxf.end.z)\n",
    "            else:\n",
    "                end = (line.dxf.end.x, line.dxf.end.y, 0.0)\n",
    "        elif hasattr(line, 'start_point') and hasattr(line, 'end_point'):\n",
    "            start = line.start_point\n",
    "            end = line.end_point\n",
    "        else:\n",
    "            logger.warning(f\"Couldn't determine line coordinates structure: {line}\")\n",
    "            start = (0, 0, 0)\n",
    "            end = (0, 0, 0)\n",
    "    elif hasattr(line, 'start') and hasattr(line, 'end'):\n",
    "        # Handle custom Line objects with simple start/end attributes\n",
    "        start_point = line.start\n",
    "        end_point = line.end\n",
    "        \n",
    "        if isinstance(start_point, tuple):\n",
    "            start = (start_point[0], start_point[1], start_point[2] if len(start_point) > 2 else 0.0)\n",
    "        else:\n",
    "            start = (start_point.x, start_point.y, start_point.z if hasattr(start_point, 'z') else 0.0)\n",
    "        \n",
    "        if isinstance(end_point, tuple):\n",
    "            end = (end_point[0], end_point[1], end_point[2] if len(end_point) > 2 else 0.0)\n",
    "        else:\n",
    "            end = (end_point.x, end_point.y, end_point.z if hasattr(end_point, 'z') else 0.0)\n",
    "    else:\n",
    "        # If we can't determine the coordinates, use fallbacks\n",
    "        logger.warning(f\"Couldn't extract coordinates for line: {line}\")\n",
    "        start = (0, 0, 0)\n",
    "        end = (1, 1, 0)\n",
    "    \n",
    "    return start, end\n",
    "\n",
    "def get_arc_data(arc) -> Tuple[Tuple[float, float, float], float, float, float]:\n",
    "    \"\"\"Get center, radius, start angle, and end angle of an arc.\"\"\"\n",
    "    try:\n",
    "        if hasattr(arc, 'dxf'):\n",
    "            center = (arc.dxf.center.x, arc.dxf.center.y, arc.dxf.center.z if hasattr(arc.dxf.center, 'z') else 0.0)\n",
    "            radius = arc.dxf.radius\n",
    "            start_angle = arc.dxf.start_angle  # In degrees\n",
    "            end_angle = arc.dxf.end_angle  # In degrees\n",
    "            return center, radius, start_angle, end_angle\n",
    "        else:\n",
    "            logger.warning(f\"Couldn't determine arc data structure: {arc}\")\n",
    "            return (0, 0, 0), 1.0, 0.0, 90.0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting arc data: {e}\")\n",
    "        return (0, 0, 0), 1.0, 0.0, 90.0\n",
    "\n",
    "def get_arc_point(center, radius, angle_deg):\n",
    "    \"\"\"Calculate point on arc at a given angle in degrees.\"\"\"\n",
    "    angle_rad = math.radians(angle_deg)\n",
    "    x = center[0] + radius * math.cos(angle_rad)\n",
    "    y = center[1] + radius * math.sin(angle_rad)\n",
    "    return (x, y, center[2])\n",
    "\n",
    "def get_arc_endpoints(arc) -> Tuple[Tuple[float, float, float], Tuple[float, float, float]]:\n",
    "    \"\"\"Get start and end points of an arc.\"\"\"\n",
    "    center, radius, start_angle, end_angle = get_arc_data(arc)\n",
    "    \n",
    "    start_point = get_arc_point(center, radius, start_angle)\n",
    "    end_point = get_arc_point(center, radius, end_angle)\n",
    "    \n",
    "    return start_point, end_point\n",
    "\n",
    "def get_arc_bbox(arc) -> BoundingBox:\n",
    "    \"\"\"Calculate the bounding box of an arc.\"\"\"\n",
    "    center, radius, start_angle, end_angle = get_arc_data(arc)\n",
    "    \n",
    "    # Ensure that start_angle is less than end_angle\n",
    "    if start_angle > end_angle:\n",
    "        end_angle += 360\n",
    "\n",
    "    # Check if the arc crosses the 0, 90, 180, or 270-degree points\n",
    "    crosses = [0, 90, 180, 270]\n",
    "    points = []\n",
    "    \n",
    "    # Add start and end points\n",
    "    points.append(get_arc_point(center, radius, start_angle))\n",
    "    points.append(get_arc_point(center, radius, end_angle))\n",
    "    \n",
    "    # Add extreme points if arc crosses them\n",
    "    for angle in crosses:\n",
    "        if start_angle <= angle <= end_angle or (start_angle > end_angle and (angle <= end_angle or angle >= start_angle)):\n",
    "            points.append(get_arc_point(center, radius, angle))\n",
    "    \n",
    "    return BoundingBox.from_points(points)\n",
    "\n",
    "def get_line_bbox(line) -> BoundingBox:\n",
    "    start, end = get_line_coords(line)\n",
    "    return BoundingBox.from_points([start, end])\n",
    "\n",
    "def get_entity_bbox(entity) -> BoundingBox:\n",
    "    \"\"\"Get bounding box for any supported entity.\"\"\"\n",
    "    if hasattr(entity, 'dxftype'):\n",
    "        entity_type = entity.dxftype()\n",
    "        if entity_type == 'LINE':\n",
    "            return get_line_bbox(entity)\n",
    "        elif entity_type == 'ARC':\n",
    "            return get_arc_bbox(entity)\n",
    "        elif entity_type in ['CIRCLE', 'ELLIPSE']:\n",
    "            # For circles and ellipses, would need custom bounding box calculation\n",
    "            # This is a placeholder until we implement full support\n",
    "            return BoundingBox(0, 0, 0, 0, 0, 0)\n",
    "    \n",
    "    # Default fallback\n",
    "    logger.warning(f\"Unknown entity type for bounding box: {entity}\")\n",
    "    return BoundingBox(0, 0, 0, 0, 0, 0)\n",
    "\n",
    "def lines_completely_overlap(line1, line2, tolerance: float) -> bool:\n",
    "    l1_start, l1_end = get_line_coords(line1)\n",
    "    l2_start, l2_end = get_line_coords(line2)\n",
    "    \n",
    "    if not are_lines_colinear(l1_start, l1_end, l2_start, l2_end, tolerance):\n",
    "        return False\n",
    "    \n",
    "    same_direction = (is_same_point(l1_start, l2_start, tolerance) and \n",
    "                     is_same_point(l1_end, l2_end, tolerance))\n",
    "    reverse_direction = (is_same_point(l1_start, l2_end, tolerance) and \n",
    "                        is_same_point(l1_end, l2_start, tolerance))\n",
    "    \n",
    "    return same_direction or reverse_direction\n",
    "\n",
    "def create_line_from_segment(msp, start_point, end_point, dxfattribs=None):\n",
    "    try:\n",
    "        return msp.add_line(start=start_point, end=end_point, dxfattribs=dxfattribs)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error creating line with attributes: {e}\")\n",
    "        return msp.add_line(start=start_point, end=end_point)\n",
    "\n",
    "def create_arc_from_parameters(msp, center, radius, start_angle, end_angle, dxfattribs=None):\n",
    "    try:\n",
    "        return msp.add_arc(center=center, radius=radius, start_angle=start_angle, end_angle=end_angle, dxfattribs=dxfattribs)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error creating arc with attributes: {e}\")\n",
    "        return msp.add_arc(center=center, radius=radius, start_angle=start_angle, end_angle=end_angle)\n",
    "\n",
    "def detect_duplicates(entities, tolerance):\n",
    "    \"\"\"Detect duplicate entities (lines, arcs, etc.)\"\"\"\n",
    "    duplicates = set()\n",
    "    entity_info = {}\n",
    "    \n",
    "    for entity in entities:\n",
    "        handle = get_entity_handle(entity)\n",
    "        \n",
    "        if hasattr(entity, 'dxftype'):\n",
    "            entity_type = entity.dxftype()\n",
    "            \n",
    "            if entity_type == 'LINE':\n",
    "                start, end = get_line_coords(entity)\n",
    "                \n",
    "                if start > end:  # Ensure consistent ordering\n",
    "                    start, end = end, start\n",
    "                    \n",
    "                # Create a key that is tolerant to small differences\n",
    "                line_key = (\n",
    "                    'LINE',\n",
    "                    round(start[0] / tolerance) * tolerance,\n",
    "                    round(start[1] / tolerance) * tolerance,\n",
    "                    round(start[2] / tolerance) * tolerance,\n",
    "                    round(end[0] / tolerance) * tolerance,\n",
    "                    round(end[1] / tolerance) * tolerance,\n",
    "                    round(end[2] / tolerance) * tolerance,\n",
    "                )\n",
    "                \n",
    "                if line_key in entity_info:\n",
    "                    duplicates.add(handle)\n",
    "                else:\n",
    "                    entity_info[line_key] = handle\n",
    "            \n",
    "            elif entity_type == 'ARC':\n",
    "                center, radius, start_angle, end_angle = get_arc_data(entity)\n",
    "                \n",
    "                # Create a key that is tolerant to small differences\n",
    "                arc_key = (\n",
    "                    'ARC',\n",
    "                    round(center[0] / tolerance) * tolerance,\n",
    "                    round(center[1] / tolerance) * tolerance,\n",
    "                    round(center[2] / tolerance) * tolerance,\n",
    "                    round(radius / tolerance) * tolerance,\n",
    "                    round(start_angle / tolerance) * tolerance,\n",
    "                    round(end_angle / tolerance) * tolerance,\n",
    "                )\n",
    "                \n",
    "                if arc_key in entity_info:\n",
    "                    duplicates.add(handle)\n",
    "                else:\n",
    "                    entity_info[arc_key] = handle\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "def build_mergeable_graph(entities, tolerance):\n",
    "    \"\"\"Build a graph of entities that can potentially be merged.\"\"\"\n",
    "    graph = defaultdict(set)\n",
    "    entity_info = {}\n",
    "    handle_to_entity = {}\n",
    "    \n",
    "    # Use a spatial index for efficient neighbor finding\n",
    "    cell_size = max(1.0, tolerance * 1000)\n",
    "    spatial_index = GridIndex(cell_size=cell_size)\n",
    "    \n",
    "    logger.info(\"Building spatial index...\")\n",
    "    for entity in entities:\n",
    "        handle = get_entity_handle(entity)\n",
    "        \n",
    "        if hasattr(entity, 'dxftype'):\n",
    "            entity_type = entity.dxftype()\n",
    "            \n",
    "            if entity_type == 'LINE':\n",
    "                start, end = get_line_coords(entity)\n",
    "                entity_info[handle] = {'type': 'LINE', 'coords': (start, end)}\n",
    "                handle_to_entity[handle] = entity\n",
    "                \n",
    "                bbox = get_line_bbox(entity)\n",
    "                spatial_index.insert(entity, bbox)\n",
    "            \n",
    "            elif entity_type == 'ARC':\n",
    "                center, radius, start_angle, end_angle = get_arc_data(entity)\n",
    "                start_point, end_point = get_arc_endpoints(entity)\n",
    "                \n",
    "                entity_info[handle] = {\n",
    "                    'type': 'ARC', \n",
    "                    'data': (center, radius, start_angle, end_angle),\n",
    "                    'endpoints': (start_point, end_point)\n",
    "                }\n",
    "                handle_to_entity[handle] = entity\n",
    "                \n",
    "                bbox = get_arc_bbox(entity)\n",
    "                spatial_index.insert(entity, bbox)\n",
    "    \n",
    "    logger.info(\"Building mergeable entity graph...\")\n",
    "    entity_count = len(entities)\n",
    "    processed = 0\n",
    "    \n",
    "    for entity in entities:\n",
    "        processed += 1\n",
    "        if processed % 1000 == 0:\n",
    "            logger.info(f\"Processing entity {processed}/{entity_count}...\")\n",
    "        \n",
    "        handle = get_entity_handle(entity)\n",
    "        if handle not in entity_info:\n",
    "            continue\n",
    "        \n",
    "        entity_data = entity_info[handle]\n",
    "        entity_type = entity_data['type']\n",
    "        \n",
    "        bbox = get_entity_bbox(entity)\n",
    "        expanded_bbox = BoundingBox(\n",
    "            bbox.min_x - tolerance,\n",
    "            bbox.min_y - tolerance,\n",
    "            bbox.min_z - tolerance,\n",
    "            bbox.max_x + tolerance,\n",
    "            bbox.max_y + tolerance,\n",
    "            bbox.max_z + tolerance\n",
    "        )\n",
    "        \n",
    "        potential_matches = spatial_index.query(expanded_bbox)\n",
    "        \n",
    "        for other_entity in potential_matches:\n",
    "            other_handle = get_entity_handle(other_entity)\n",
    "                \n",
    "            if other_handle == handle:\n",
    "                continue\n",
    "            \n",
    "            if other_handle not in entity_info:\n",
    "                continue\n",
    "                \n",
    "            if other_handle in graph[handle] or handle in graph[other_handle]:\n",
    "                continue\n",
    "            \n",
    "            other_data = entity_info[other_handle]\n",
    "            other_type = other_data['type']\n",
    "            \n",
    "            # Skip if entities are of different types\n",
    "            if entity_type != other_type:\n",
    "                continue\n",
    "            \n",
    "            # Handle LINE entities\n",
    "            if entity_type == 'LINE':\n",
    "                start, end = entity_data['coords']\n",
    "                other_start, other_end = other_data['coords']\n",
    "                \n",
    "                # Check if points are close enough\n",
    "                if point_distance(start, other_start) > tolerance and \\\n",
    "                   point_distance(start, other_end) > tolerance and \\\n",
    "                   point_distance(end, other_start) > tolerance and \\\n",
    "                   point_distance(end, other_end) > tolerance:\n",
    "                    \n",
    "                    min_dist = min(\n",
    "                        point_to_line_distance(start, other_start, other_end),\n",
    "                        point_to_line_distance(end, other_start, other_end),\n",
    "                        point_to_line_distance(other_start, start, end),\n",
    "                        point_to_line_distance(other_end, start, end)\n",
    "                    )\n",
    "                    \n",
    "                    if min_dist > tolerance:\n",
    "                        continue\n",
    "                \n",
    "                if are_lines_colinear(start, end, other_start, other_end, tolerance):\n",
    "                    has_overlap, _ = lines_overlap(start, end, other_start, other_end, tolerance)\n",
    "                    \n",
    "                    if has_overlap:\n",
    "                        graph[handle].add(other_handle)\n",
    "                        graph[other_handle].add(handle)\n",
    "                        continue\n",
    "                    \n",
    "                    connections = [\n",
    "                        (start, other_start),\n",
    "                        (start, other_end),\n",
    "                        (end, other_start),\n",
    "                        (end, other_end)\n",
    "                    ]\n",
    "                    \n",
    "                    for p1, p2 in connections:\n",
    "                        if is_same_point(p1, p2, tolerance):\n",
    "                            graph[handle].add(other_handle)\n",
    "                            graph[other_handle].add(handle)\n",
    "                            break\n",
    "            \n",
    "            # Handle ARC entities\n",
    "            elif entity_type == 'ARC':\n",
    "                entity_center, entity_radius, entity_start_angle, entity_end_angle = entity_data['data']\n",
    "                other_center, other_radius, other_start_angle, other_end_angle = other_data['data']\n",
    "                \n",
    "                # Check if arcs have same center and radius (within tolerance)\n",
    "                center_distance = point_distance(entity_center, other_center)\n",
    "                radius_difference = abs(entity_radius - other_radius)\n",
    "                \n",
    "                if center_distance > tolerance or radius_difference > tolerance:\n",
    "                    continue\n",
    "                \n",
    "                # Normalize angles to 0-360 range\n",
    "                entity_start_angle = entity_start_angle % 360\n",
    "                entity_end_angle = entity_end_angle % 360\n",
    "                other_start_angle = other_start_angle % 360\n",
    "                other_end_angle = other_end_angle % 360\n",
    "                \n",
    "                # If end angle is less than start angle, add 360 to make it easier to check overlap\n",
    "                if entity_end_angle < entity_start_angle:\n",
    "                    entity_end_angle += 360\n",
    "                if other_end_angle < other_start_angle:\n",
    "                    other_end_angle += 360\n",
    "                \n",
    "                # Check if arcs overlap or are adjacent\n",
    "                if (entity_start_angle <= other_end_angle + tolerance and \n",
    "                    entity_end_angle + tolerance >= other_start_angle):\n",
    "                    graph[handle].add(other_handle)\n",
    "                    graph[other_handle].add(handle)\n",
    "                    continue\n",
    "                \n",
    "                # Check if arcs are adjacent (within tolerance)\n",
    "                if (abs(entity_start_angle - other_end_angle) <= tolerance or\n",
    "                    abs(entity_end_angle - other_start_angle) <= tolerance):\n",
    "                    graph[handle].add(other_handle)\n",
    "                    graph[other_handle].add(handle)\n",
    "    \n",
    "    return graph, entity_info, handle_to_entity\n",
    "\n",
    "def find_connected_components(graph):\n",
    "    \"\"\"Find connected components in the graph.\"\"\"\n",
    "    visited = set()\n",
    "    components = []\n",
    "    \n",
    "    for node in graph:\n",
    "        if node in visited:\n",
    "            continue\n",
    "        \n",
    "        component = set()\n",
    "        queue = deque([node])\n",
    "        \n",
    "        while queue:\n",
    "            current = queue.popleft()\n",
    "            if current in visited:\n",
    "                continue\n",
    "                \n",
    "            visited.add(current)\n",
    "            component.add(current)\n",
    "            \n",
    "            for neighbor in graph[current]:\n",
    "                if neighbor not in visited:\n",
    "                    queue.append(neighbor)\n",
    "        \n",
    "        components.append(component)\n",
    "    \n",
    "    return components\n",
    "\n",
    "def can_merge_arcs(arc1_data, arc2_data, tolerance):\n",
    "    \"\"\"Check if two arcs can be merged into a single arc.\"\"\"\n",
    "    center1, radius1, start_angle1, end_angle1 = arc1_data\n",
    "    center2, radius2, start_angle2, end_angle2 = arc2_data\n",
    "    \n",
    "    # Normalize angles to 0-360 range\n",
    "    start_angle1 = start_angle1 % 360\n",
    "    end_angle1 = end_angle1 % 360\n",
    "    start_angle2 = start_angle2 % 360\n",
    "    end_angle2 = end_angle2 % 360\n",
    "    \n",
    "    # If end angle is less than start angle, add 360\n",
    "    if end_angle1 < start_angle1:\n",
    "        end_angle1 += 360\n",
    "    if end_angle2 < start_angle2:\n",
    "        end_angle2 += 360\n",
    "    \n",
    "    # Check if centers and radii are close enough\n",
    "    center_distance = point_distance(center1, center2)\n",
    "    radius_difference = abs(radius1 - radius2)\n",
    "    \n",
    "    if center_distance > tolerance or radius_difference > tolerance:\n",
    "        return False\n",
    "    \n",
    "    # Check if arcs are adjacent or overlapping\n",
    "    if abs(start_angle1 - end_angle2) <= tolerance or abs(end_angle1 - start_angle2) <= tolerance:\n",
    "        return True\n",
    "    \n",
    "    # Check if one arc contains the other\n",
    "    if (start_angle1 - tolerance <= start_angle2 <= end_angle1 + tolerance or\n",
    "        start_angle1 - tolerance <= end_angle2 <= end_angle1 + tolerance or\n",
    "        start_angle2 - tolerance <= start_angle1 <= end_angle2 + tolerance or\n",
    "        start_angle2 - tolerance <= end_angle1 <= end_angle2 + tolerance):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def merge_arcs(arc_datas, tolerance):\n",
    "    \"\"\"Merge a list of arc data into one or more arcs.\"\"\"\n",
    "    if not arc_datas:\n",
    "        return []\n",
    "    \n",
    "    # Group arcs by center and radius\n",
    "    groups = {}\n",
    "    for center, radius, start_angle, end_angle in arc_datas:\n",
    "        key = (round(center[0]/tolerance), round(center[1]/tolerance), round(center[2]/tolerance), round(radius/tolerance))\n",
    "        if key not in groups:\n",
    "            groups[key] = []\n",
    "        groups[key].append((center, radius, start_angle, end_angle))\n",
    "    \n",
    "    result_arcs = []\n",
    "    \n",
    "    for center_radius_group in groups.values():\n",
    "        # Normalize all angles to 0-360 range\n",
    "        for i in range(len(center_radius_group)):\n",
    "            center, radius, start_angle, end_angle = center_radius_group[i]\n",
    "            start_angle = start_angle % 360\n",
    "            end_angle = end_angle % 360\n",
    "            if end_angle < start_angle:\n",
    "                end_angle += 360\n",
    "            center_radius_group[i] = (center, radius, start_angle, end_angle)\n",
    "        \n",
    "        # Sort by start angle\n",
    "        center_radius_group.sort(key=lambda x: x[2])\n",
    "        \n",
    "        # Merge arcs\n",
    "        merged_arcs = []\n",
    "        current_arc = center_radius_group[0]\n",
    "        \n",
    "        for next_arc in center_radius_group[1:]:\n",
    "            center1, radius1, start_angle1, end_angle1 = current_arc\n",
    "            center2, radius2, start_angle2, end_angle2 = next_arc\n",
    "            \n",
    "            # If the next arc's start angle is within tolerance of the current arc's end angle,\n",
    "            # or if the next arc starts within the current arc's angle range, merge them\n",
    "            if end_angle1 + tolerance >= start_angle2:\n",
    "                current_arc = (center1, radius1, start_angle1, max(end_angle1, end_angle2))\n",
    "            else:\n",
    "                merged_arcs.append(current_arc)\n",
    "                current_arc = next_arc\n",
    "        \n",
    "        merged_arcs.append(current_arc)\n",
    "        \n",
    "        # Normalize merged arcs' angles back to DXF convention\n",
    "        for center, radius, start_angle, end_angle in merged_arcs:\n",
    "            # Ensure angles are in 0-360 range\n",
    "            start_angle = start_angle % 360\n",
    "            end_angle = end_angle % 360\n",
    "            \n",
    "            # If the arc spans more than 360 degrees, create a full circle\n",
    "            if end_angle - start_angle >= 360 - tolerance:\n",
    "                start_angle = 0\n",
    "                end_angle = 360\n",
    "            \n",
    "            result_arcs.append((center, radius, start_angle, end_angle))\n",
    "    \n",
    "    return result_arcs\n",
    "\n",
    "def merge_component_entities(component, entity_info, handle_to_entity, msp, tolerance):\n",
    "    \"\"\"Merge a connected component of entities.\"\"\"\n",
    "    if not component:\n",
    "        return [], set()\n",
    "    \n",
    "    # Group entities by type\n",
    "    lines = []\n",
    "    arcs = []\n",
    "    delete_handles = set()\n",
    "    \n",
    "    for handle in component:\n",
    "        if handle in entity_info:\n",
    "            entity_data = entity_info[handle]\n",
    "            entity_type = entity_data['type']\n",
    "            \n",
    "            if entity_type == 'LINE':\n",
    "                start, end = entity_data['coords']\n",
    "                lines.append((start, end))\n",
    "            elif entity_type == 'ARC':\n",
    "                arc_data = entity_data['data']\n",
    "                arcs.append(arc_data)\n",
    "            \n",
    "            delete_handles.add(handle)\n",
    "    \n",
    "    new_entities = []\n",
    "    \n",
    "    # Process lines\n",
    "    if lines:\n",
    "        ref_handle = next(iter(component))\n",
    "        dxfattribs = get_entity_dxf_attribs(handle_to_entity[ref_handle])\n",
    "        \n",
    "        all_points = []\n",
    "        for start, end in lines:\n",
    "            all_points.append(start)\n",
    "            all_points.append(end)\n",
    "        \n",
    "        if not all_points:\n",
    "            pass  # Skip if no points\n",
    "        else:\n",
    "            # Find a reference vector to project points onto\n",
    "            ref_start, ref_end = lines[0]\n",
    "            ref_vector = Vector.from_points(ref_start, ref_end)\n",
    "            \n",
    "            if ref_vector.length() == 0:\n",
    "                different_points = []\n",
    "                for i, p1 in enumerate(all_points):\n",
    "                    for p2 in all_points[i+1:]:\n",
    "                        if not is_same_point(p1, p2, tolerance):\n",
    "                            different_points = [p1, p2]\n",
    "                            break\n",
    "                    if different_points:\n",
    "                        break\n",
    "                \n",
    "                if not different_points:\n",
    "                    new_line = create_line_from_segment(msp, ref_start, ref_start, dxfattribs)\n",
    "                    new_entities.append(new_line)\n",
    "                else:\n",
    "                    ref_start, ref_end = different_points\n",
    "                    ref_vector = Vector.from_points(ref_start, ref_end)\n",
    "            \n",
    "            if ref_vector.length() > 0:\n",
    "                ref_vector = ref_vector.normalize()\n",
    "                \n",
    "                # Project all points onto the reference vector\n",
    "                projected_points = []\n",
    "                for point in all_points:\n",
    "                    point_vector = Vector.from_points(ref_start, point)\n",
    "                    projection = point_vector.dot(ref_vector)\n",
    "                    projected_points.append((projection, point))\n",
    "                \n",
    "                projected_points.sort(key=lambda x: x[0])\n",
    "                \n",
    "                # Create a list of unique points\n",
    "                unique_points = []\n",
    "                for _, point in projected_points:\n",
    "                    is_duplicate = False\n",
    "                    for existing in unique_points:\n",
    "                        if is_same_point(existing, point, tolerance):\n",
    "                            is_duplicate = True\n",
    "                            break\n",
    "                    \n",
    "                    if not is_duplicate:\n",
    "                        unique_points.append(point)\n",
    "                \n",
    "                # Filter out collinear middle points\n",
    "                filtered_points = []\n",
    "                if len(unique_points) <= 2:\n",
    "                    filtered_points = unique_points\n",
    "                else:\n",
    "                    filtered_points = [unique_points[0]]\n",
    "                    \n",
    "                    for i in range(1, len(unique_points) - 1):\n",
    "                        p_prev = unique_points[i-1]\n",
    "                        p_curr = unique_points[i]\n",
    "                        p_next = unique_points[i+1]\n",
    "                        \n",
    "                        if not are_collinear(p_prev, p_curr, p_next, tolerance):\n",
    "                            filtered_points.append(p_curr)\n",
    "                    \n",
    "                    filtered_points.append(unique_points[-1])\n",
    "                \n",
    "                # Create new line segments\n",
    "                for i in range(len(filtered_points) - 1):\n",
    "                    start = filtered_points[i]\n",
    "                    end = filtered_points[i + 1]\n",
    "                    \n",
    "                    if not is_same_point(start, end, tolerance):\n",
    "                        new_line = create_line_from_segment(msp, start, end, dxfattribs)\n",
    "                        new_entities.append(new_line)\n",
    "    \n",
    "    # Process arcs\n",
    "    if arcs:\n",
    "        ref_handle = next(iter(component))\n",
    "        dxfattribs = get_entity_dxf_attribs(handle_to_entity[ref_handle])\n",
    "        \n",
    "        merged_arcs = merge_arcs(arcs, tolerance)\n",
    "        \n",
    "        for center, radius, start_angle, end_angle in merged_arcs:\n",
    "            # Check if this is a full circle\n",
    "            if abs(end_angle - start_angle - 360) < tolerance or abs(end_angle - start_angle) < tolerance:\n",
    "                # Create a circle instead of an arc\n",
    "                try:\n",
    "                    new_circle = msp.add_circle(center=center, radius=radius, dxfattribs=dxfattribs)\n",
    "                    new_entities.append(new_circle)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error creating circle: {e}\")\n",
    "                    # Fallback to creating an arc\n",
    "                    new_arc = create_arc_from_parameters(msp, center, radius, 0, 360, dxfattribs)\n",
    "                    new_entities.append(new_arc)\n",
    "            else:\n",
    "                new_arc = create_arc_from_parameters(msp, center, radius, start_angle, end_angle, dxfattribs)\n",
    "                new_entities.append(new_arc)\n",
    "    \n",
    "    return new_entities, delete_handles\n",
    "\n",
    "def optimize_polyline_segments(polyline, tolerance: float) -> bool:\n",
    "    \"\"\"Optimize a polyline by removing redundant vertices.\"\"\"\n",
    "    try:\n",
    "        if polyline.dxftype() == 'LWPOLYLINE':\n",
    "            vertices = []\n",
    "            for i in range(len(polyline)):\n",
    "                point = polyline.get_point(i)\n",
    "                vertices.append((point[0], point[1], 0.0))\n",
    "        elif polyline.dxftype() == 'POLYLINE':\n",
    "            vertices = []\n",
    "            for vertex in polyline.vertices():\n",
    "                if hasattr(vertex, 'dxf'):\n",
    "                    x = vertex.dxf.location[0]\n",
    "                    y = vertex.dxf.location[1]\n",
    "                    z = vertex.dxf.location[2] if len(vertex.dxf.location) > 2 else 0.0\n",
    "                    vertices.append((x, y, z))\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        if len(vertices) < 3:\n",
    "            return False\n",
    "        \n",
    "        # Identify collinear middle points\n",
    "        to_remove = []\n",
    "        for i in range(1, len(vertices) - 1):\n",
    "            if are_collinear(vertices[i-1], vertices[i], vertices[i+1], tolerance):\n",
    "                to_remove.append(i)\n",
    "        \n",
    "        # If we found collinear points, create a new polyline without them\n",
    "        if to_remove:\n",
    "            # This would require modifying the polyline, which is complex\n",
    "            # For now, just return whether we found optimizations\n",
    "            return len(to_remove) > 0\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error optimizing polyline: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_all_entities_by_type(msp, entity_type):\n",
    "    \"\"\"Get all entities of a specific type from the modelspace.\"\"\"\n",
    "    return [entity for entity in msp if entity.dxftype() == entity_type]\n",
    "\n",
    "def destroy_entity_safely(entity):\n",
    "    \"\"\"Safely destroy an entity.\"\"\"\n",
    "    try:\n",
    "        if hasattr(entity, 'destroy'):\n",
    "            entity.destroy()\n",
    "            return True\n",
    "        else:\n",
    "            logger.warning(f\"Entity does not have destroy method: {entity}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error destroying entity: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_polylines_by_floor_layers(msp):\n",
    "    \"\"\"Extract all polylines from Floor layers (Floor 1, Floor 2, etc.)\"\"\"\n",
    "    floor_polylines = []\n",
    "    \n",
    "    for entity in msp:\n",
    "        if entity.dxftype() in ['LWPOLYLINE', 'POLYLINE']:\n",
    "            if hasattr(entity, 'dxf') and hasattr(entity.dxf, 'layer'):\n",
    "                layer_name = entity.dxf.layer\n",
    "                if layer_name.startswith('Floor ') and layer_name[6:].strip().isdigit():\n",
    "                    # Create a copy of the polyline attributes to restore later\n",
    "                    polyline_data = {\n",
    "                        'entity': entity,\n",
    "                        'type': entity.dxftype(),\n",
    "                        'layer': layer_name,\n",
    "                        'handle': get_entity_handle(entity),\n",
    "                        'dxfattribs': get_entity_dxf_attribs(entity)\n",
    "                    }\n",
    "                    \n",
    "                    # Extract vertices data\n",
    "                    vertices = []\n",
    "                    try:\n",
    "                        if entity.dxftype() == 'LWPOLYLINE':\n",
    "                            for i in range(len(entity)):\n",
    "                                point = entity.get_point(i)\n",
    "                                bulge = entity.get_bulge(i)\n",
    "                                vertices.append((point[0], point[1], bulge))\n",
    "                            \n",
    "                            # Get closed status\n",
    "                            polyline_data['closed'] = entity.closed\n",
    "                            \n",
    "                        elif entity.dxftype() == 'POLYLINE':\n",
    "                            for vertex in entity.vertices():\n",
    "                                if hasattr(vertex, 'dxf'):\n",
    "                                    x = vertex.dxf.location[0]\n",
    "                                    y = vertex.dxf.location[1]\n",
    "                                    z = vertex.dxf.location[2] if len(vertex.dxf.location) > 2 else 0.0\n",
    "                                    bulge = vertex.dxf.bulge if hasattr(vertex.dxf, 'bulge') else 0.0\n",
    "                                    vertices.append((x, y, z, bulge))\n",
    "                            \n",
    "                            # Get closed status\n",
    "                            polyline_data['closed'] = entity.is_closed\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error extracting polyline data: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    polyline_data['vertices'] = vertices\n",
    "                    floor_polylines.append(polyline_data)\n",
    "    \n",
    "    logger.info(f\"Found {len(floor_polylines)} polylines in Floor layers\")\n",
    "    return floor_polylines\n",
    "\n",
    "def restore_floor_polylines(msp, floor_polylines):\n",
    "    \"\"\"Restore the preserved floor polylines to the drawing\"\"\"\n",
    "    restored_count = 0\n",
    "    \n",
    "    for polyline_data in floor_polylines:\n",
    "        try:\n",
    "            if polyline_data['type'] == 'LWPOLYLINE':\n",
    "                # Create a new lightweight polyline\n",
    "                points = [(v[0], v[1]) for v in polyline_data['vertices']]\n",
    "                bulges = [v[2] for v in polyline_data['vertices']]\n",
    "                \n",
    "                new_polyline = msp.add_lwpolyline(\n",
    "                    points, \n",
    "                    dxfattribs=polyline_data['dxfattribs'],\n",
    "                    close=polyline_data['closed']\n",
    "                )\n",
    "                \n",
    "                # Set bulges\n",
    "                for i, bulge in enumerate(bulges):\n",
    "                    if bulge != 0:\n",
    "                        new_polyline.set_bulge(i, bulge)\n",
    "                \n",
    "            elif polyline_data['type'] == 'POLYLINE':\n",
    "                # Create a new polyline\n",
    "                new_polyline = msp.add_polyline3d(\n",
    "                    [], \n",
    "                    dxfattribs=polyline_data['dxfattribs'],\n",
    "                    close=polyline_data['closed']\n",
    "                )\n",
    "                \n",
    "                # Add vertices\n",
    "                for vertex_data in polyline_data['vertices']:\n",
    "                    if len(vertex_data) == 4:  # (x, y, z, bulge)\n",
    "                        x, y, z, bulge = vertex_data\n",
    "                        vertex = new_polyline.add_vertex((x, y, z))\n",
    "                        if hasattr(vertex, 'dxf') and bulge != 0:\n",
    "                            vertex.dxf.bulge = bulge\n",
    "            \n",
    "            restored_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error restoring polyline: {e}\")\n",
    "    \n",
    "    logger.info(f\"Restored {restored_count} polylines to Floor layers\")\n",
    "    return restored_count\n",
    "\n",
    "def remove_entities_safely(entities_list):\n",
    "    \"\"\"Remove entities from the drawing safely\"\"\"\n",
    "    removed_count = 0\n",
    "    for entity in entities_list:\n",
    "        if destroy_entity_safely(entity):\n",
    "            removed_count += 1\n",
    "    return removed_count\n",
    "\n",
    "def overkill(dxf_file_path: str, output_file_path: str = None, tolerance: float = 1e-6, max_iterations: int = 10) -> None:\n",
    "    \"\"\"Main function to perform the OVERKILL operation on a DXF file.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if output_file_path is None:\n",
    "        output_file_path = dxf_file_path\n",
    "    \n",
    "    try:\n",
    "        doc = ezdxf.readfile(dxf_file_path)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading DXF file: {e}\")\n",
    "        return\n",
    "    \n",
    "    msp = doc.modelspace()\n",
    "    \n",
    "    # First, identify and save polylines from Floor layers\n",
    "    logger.info(\"Identifying polylines in Floor layers...\")\n",
    "    floor_polylines = get_polylines_by_floor_layers(msp)\n",
    "    \n",
    "    # Remove the Floor layer polylines temporarily\n",
    "    removed_floor_polylines = 0\n",
    "    if floor_polylines:\n",
    "        logger.info(\"Temporarily removing polylines from Floor layers...\")\n",
    "        removed_floor_polylines = remove_entities_safely([data['entity'] for data in floor_polylines])\n",
    "        logger.info(f\"Temporarily removed {removed_floor_polylines} polylines from Floor layers\")\n",
    "    \n",
    "    total_deleted = 0\n",
    "    total_created = 0\n",
    "    original_entity_count = 0\n",
    "    \n",
    "    logger.info(\"Starting OVERKILL process...\")\n",
    "    \n",
    "    iteration = 0\n",
    "    changes_made = True\n",
    "    \n",
    "    while changes_made and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        logger.info(f\"\\n--- Iteration {iteration} ---\")\n",
    "        \n",
    "        # Get all supported entities\n",
    "        lines = get_all_entities_by_type(msp, 'LINE')\n",
    "        arcs = get_all_entities_by_type(msp, 'ARC')\n",
    "        \n",
    "        entities = lines + arcs\n",
    "        \n",
    "        if iteration == 1:\n",
    "            original_entity_count = len(entities)\n",
    "            \n",
    "        if len(entities) == 0:\n",
    "            logger.info(\"No supported entities found in the drawing.\")\n",
    "            break\n",
    "            \n",
    "        logger.info(f\"Processing {len(entities)} entities \"\n",
    "                   f\"({len(lines)} lines, {len(arcs)} arcs)...\")\n",
    "        \n",
    "        # Detect and remove duplicates\n",
    "        duplicate_handles = detect_duplicates(entities, tolerance)\n",
    "        \n",
    "        duplicates_deleted = 0\n",
    "        for handle in duplicate_handles:\n",
    "            for entity in entities:\n",
    "                entity_handle = get_entity_handle(entity)\n",
    "                if entity_handle == handle:\n",
    "                    destroy_entity_safely(entity)\n",
    "                    duplicates_deleted += 1\n",
    "                    break\n",
    "        \n",
    "        if duplicates_deleted > 0:\n",
    "            logger.info(f\"Removed {duplicates_deleted} duplicate entities.\")\n",
    "            total_deleted += duplicates_deleted\n",
    "            \n",
    "            # Refresh the entity list\n",
    "            lines = get_all_entities_by_type(msp, 'LINE')\n",
    "            arcs = get_all_entities_by_type(msp, 'ARC')\n",
    "            entities = lines + arcs\n",
    "        \n",
    "        logger.info(\"Analyzing overlapping and connecting entities...\")\n",
    "        graph, entity_info, handle_to_entity = build_mergeable_graph(entities, tolerance)\n",
    "        \n",
    "        components = find_connected_components(graph)\n",
    "        \n",
    "        mergeable_components = [comp for comp in components if len(comp) > 1]\n",
    "        logger.info(f\"Found {len(mergeable_components)} sets of entities that can be merged.\")\n",
    "        \n",
    "        if not mergeable_components:\n",
    "            logger.info(\"No more entities can be merged.\")\n",
    "            changes_made = False\n",
    "            break\n",
    "        \n",
    "        logger.info(\"Merging entity components...\")\n",
    "        iteration_new_entities = []\n",
    "        iteration_handles_to_delete = set()\n",
    "        \n",
    "        for i, component in enumerate(mergeable_components):\n",
    "            if i % 100 == 0 and i > 0:\n",
    "                logger.info(f\"Processed {i}/{len(mergeable_components)} components...\")\n",
    "            \n",
    "            component_new_entities, component_delete_handles = merge_component_entities(\n",
    "                component, entity_info, handle_to_entity, msp, tolerance\n",
    "            )\n",
    "            \n",
    "            iteration_new_entities.extend(component_new_entities)\n",
    "            iteration_handles_to_delete.update(component_delete_handles)\n",
    "        \n",
    "        entities_deleted = 0\n",
    "        for handle in iteration_handles_to_delete:\n",
    "            if handle in handle_to_entity:\n",
    "                destroy_entity_safely(handle_to_entity[handle])\n",
    "                entities_deleted += 1\n",
    "        \n",
    "        logger.info(f\"Iteration {iteration} results:\")\n",
    "        logger.info(f\"  - Entities merged/deleted: {entities_deleted}\")\n",
    "        logger.info(f\"  - New entities created: {len(iteration_new_entities)}\")\n",
    "        \n",
    "        total_deleted += entities_deleted\n",
    "        total_created += len(iteration_new_entities)\n",
    "        \n",
    "        changes_made = (entities_deleted > 0) or (duplicates_deleted > 0)\n",
    "    \n",
    "    # Restore the Floor layer polylines\n",
    "    if floor_polylines:\n",
    "        logger.info(\"Restoring polylines to Floor layers...\")\n",
    "        restored_count = restore_floor_polylines(msp, floor_polylines)\n",
    "        logger.info(f\"Restored {restored_count} polylines to Floor layers\")\n",
    "    \n",
    "    try:\n",
    "        doc.saveas(output_file_path)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        current_entity_count = len(get_all_entities_by_type(msp, 'LINE')) + len(get_all_entities_by_type(msp, 'ARC'))\n",
    "        \n",
    "        logger.info(f\"\\nOVERKILL operation completed in {execution_time:.2f} seconds:\")\n",
    "        logger.info(f\"  - Iterations performed: {iteration}\")\n",
    "        logger.info(f\"  - Original entity count: {original_entity_count}\")\n",
    "        logger.info(f\"  - Total entities deleted: {total_deleted}\")\n",
    "        logger.info(f\"  - Total new entities created: {total_created}\")\n",
    "        logger.info(f\"  - Final entity count: {current_entity_count}\")\n",
    "        logger.info(f\"  - Entity reduction: {original_entity_count - current_entity_count} entities \"\n",
    "                  f\"({((original_entity_count - current_entity_count) / original_entity_count * 100):.1f}%)\")\n",
    "        if floor_polylines:\n",
    "            logger.info(f\"  - Floor layer polylines preserved: {len(floor_polylines)}\")\n",
    "        logger.info(f\"  - Result saved to {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving DXF file: {e}\")\n",
    "\n",
    "def main0(input_dxf0, output_dxf, tolerance=1e-6, max_iterations=10):\n",
    "    \"\"\"Entry point function for the OVERKILL tool.\"\"\"\n",
    "    print(f\"Starting OVERKILL process on {input_dxf0}\")\n",
    "    print(f\"Using tolerance value: {tolerance}\")\n",
    "    print(f\"Maximum iterations: {max_iterations}\")\n",
    "    print(\"This may take a moment for complex drawings...\")\n",
    "    \n",
    "    overkill(input_dxf0, output_dxf, tolerance, max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2142f5",
   "metadata": {},
   "source": [
    "# all polylines to lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b6b6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lwpolyline_to_lines(doc):\n",
    "    modelspace = doc.modelspace()\n",
    "    new_lines = []\n",
    "    original_polylines = []\n",
    "    skipped_polylines = []\n",
    "    lwpolylines = list(modelspace.query('LWPOLYLINE'))\n",
    "    \n",
    "    for lwpolyline in lwpolylines:\n",
    "        layer = lwpolyline.dxf.layer if hasattr(lwpolyline.dxf, 'layer') else \"\"\n",
    "        \n",
    "        # Check if layer matches any of the patterns to skip\n",
    "        skip_layer = False\n",
    "        \n",
    "        # Check for \"FloorX\" or \"Floor X\" pattern (where X is a number)\n",
    "        if layer.lower().startswith(\"floor\") or layer.lower().startswith(\"floor \"):\n",
    "            layer_suffix = layer[5:].strip() if layer.lower().startswith(\"floor\") else layer[6:].strip()\n",
    "            if layer_suffix.isdigit() or (len(layer_suffix) > 0 and layer_suffix[0].isdigit()):\n",
    "                skip_layer = True\n",
    "        \n",
    "        # Check for specific named floors\n",
    "        floor_names = [\"ground floor\", \"first floor\", \"second floor\", \"third floor\", \n",
    "                      \"fourth floor\", \"fifth floor\", \"sixth floor\", \"seventh floor\", \n",
    "                      \"terrace floor\"]\n",
    "        \n",
    "        if layer.lower() in floor_names:\n",
    "            skip_layer = True\n",
    "        \n",
    "        if skip_layer:\n",
    "            skipped_polylines.append(lwpolyline)\n",
    "            continue\n",
    "        \n",
    "        original_polylines.append(lwpolyline)\n",
    "        color = lwpolyline.dxf.color if hasattr(lwpolyline.dxf, 'color') else None\n",
    "        linetype = lwpolyline.dxf.linetype if hasattr(lwpolyline.dxf, 'linetype') else None\n",
    "        \n",
    "        try:\n",
    "            points = list(lwpolyline.vertices())\n",
    "            if len(points) < 2:\n",
    "                continue\n",
    "            \n",
    "            for j in range(len(points) - 1):\n",
    "                start_point = points[j]\n",
    "                end_point = points[j + 1]\n",
    "                new_line = modelspace.add_line(start=start_point, end=end_point)\n",
    "                new_line.dxf.layer = layer\n",
    "                if color is not None:\n",
    "                    new_line.dxf.color = color\n",
    "                if linetype is not None:\n",
    "                    new_line.dxf.linetype = linetype\n",
    "                new_lines.append(new_line)\n",
    "            \n",
    "            if lwpolyline.closed:\n",
    "                new_line = modelspace.add_line(start=points[-1], end=points[0])\n",
    "                new_line.dxf.layer = layer\n",
    "                if color is not None:\n",
    "                    new_line.dxf.color = color\n",
    "                if linetype is not None:\n",
    "                    new_line.dxf.linetype = linetype\n",
    "                new_lines.append(new_line)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return new_lines, original_polylines, skipped_polylines\n",
    "\n",
    "def delete_original_polylines(doc, polylines):\n",
    "    modelspace = doc.modelspace()\n",
    "    for polyline in polylines:\n",
    "        try:\n",
    "            modelspace.delete_entity(polyline)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def verify_conversion(doc):\n",
    "    modelspace = doc.modelspace()\n",
    "    return len(list(modelspace.query('LWPOLYLINE')))\n",
    "\n",
    "def main1(input_dxf1, output_dxf, delete_originals=True):\n",
    "    try:\n",
    "        doc = ezdxf.readfile(input_dxf1)\n",
    "        new_lines, original_polylines, skipped_polylines = lwpolyline_to_lines(doc)\n",
    "        \n",
    "        # Only delete polylines that were actually converted\n",
    "        if delete_originals and original_polylines:\n",
    "            delete_original_polylines(doc, original_polylines)\n",
    "            \n",
    "        remaining_polylines = verify_conversion(doc)\n",
    "        print(f\"Converted {len(original_polylines)} polylines to {len(new_lines)} lines\")\n",
    "        print(f\"Skipped {len(skipped_polylines)} polylines on floor layers\")\n",
    "        print(f\"Remaining polylines after conversion: {remaining_polylines}\")\n",
    "        \n",
    "        doc.saveas(output_dxf)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error in conversion: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6cd1f",
   "metadata": {},
   "source": [
    "# all layers to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7afe7750-9a39-4722-919c-ece373d67498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezdxf\n",
    "\n",
    "def move_entities_to_zero_layer(doc, preserve_layers=None):\n",
    "    \"\"\"\n",
    "    Moves all entities to layer '0' except those in specified preserve_layers.\n",
    "\n",
    "    Parameters:\n",
    "    - doc: An ezdxf document object.\n",
    "    - preserve_layers: List of layers to keep unchanged (default: floor layers).\n",
    "    \"\"\"\n",
    "    if preserve_layers is None:\n",
    "        preserve_layers = [\n",
    "            \"Floor1\", \"Floor2\", \"Floor3\", \"Floor4\", \n",
    "            \"Floor5\", \"Floor 6\", \"Floor 7\", \"Floor 8\", \n",
    "            \"terrace floor\"\n",
    "        ]\n",
    "\n",
    "    for entity in doc.modelspace():\n",
    "        if entity.dxf.layer.lower() not in [layer.lower() for layer in preserve_layers]:\n",
    "            entity.dxf.layer = \"0\"  # Move entity to layer '0'\n",
    "\n",
    "def main2(input_dxf2, output_dxf):\n",
    "    \"\"\"\n",
    "    Reads a DXF file, moves entities to layer '0' except specified layers, and saves it.\n",
    "\n",
    "    Parameters:\n",
    "    - input_dxf2 (str): Path to input DXF file.\n",
    "    - output_dxf (str): Path to save the modified DXF file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = ezdxf.readfile(input_dxf2)  # Load DXF file\n",
    "        move_entities_to_zero_layer(doc)  # Move entities\n",
    "        doc.saveas(output_dxf)  # Save modified DXF\n",
    "        print(f\"DXF file saved as {output_dxf}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {input_dxf2}\")\n",
    "    except ezdxf.DXFStructureError:\n",
    "        print(f\"Error: Invalid or corrupted DXF file - {input_dxf2}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aede5a",
   "metadata": {},
   "source": [
    "# deleteing blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8efab392",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def count_and_remove_blocks(input_dxf3, output_dxf):\n",
    "    try:\n",
    "        # Load DXF file\n",
    "        print(f\"Loading DXF file: {input_dxf3}\")\n",
    "        dwg = ezdxf.readfile(input_dxf3)\n",
    "        \n",
    "        # Count blocks\n",
    "        block_counts = {}\n",
    "        for block in dwg.modelspace().query('INSERT'):\n",
    "            block_name = block.dxf.name.upper()\n",
    "            block_counts[block_name] = block_counts.get(block_name, 0) + 1\n",
    "        \n",
    "        # Print block counts\n",
    "        print(\"\\nBlock Counts:\")\n",
    "        for name, count in block_counts.items():\n",
    "            print(f\"  {name}: {count}\")\n",
    "        \n",
    "        # Process and delete blocks\n",
    "        blocks_processed = 0\n",
    "        entities_deleted = 0\n",
    "        blocks_to_process = list(dwg.modelspace().query('INSERT'))\n",
    "        \n",
    "        for entity in blocks_to_process:\n",
    "            blocks_processed += 1\n",
    "            block_name = entity.dxf.name\n",
    "            try:\n",
    "                dwg.modelspace().delete_entity(entity)\n",
    "                entities_deleted += 1\n",
    "                #print(f\"Deleted block reference: {block_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing block {block_name}: {str(e)}\")\n",
    "        \n",
    "        #print(f\"Processed {blocks_processed} blocks\")\n",
    "        #print(f\"Deleted {entities_deleted} entities\")\n",
    "        \n",
    "        # Save the modified document\n",
    "        dwg.saveas(output_dxf)\n",
    "        #print(f\"\\nSaved modified DXF file: {output_dxf}\")\n",
    "        \n",
    "        return block_counts\n",
    "    except ezdxf.DXFError as e:\n",
    "        print(f\"DXF Error: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "def main3(input_dxf3, output_dxf):\n",
    "    count_and_remove_blocks(input_dxf3, output_dxf)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f524027",
   "metadata": {},
   "source": [
    "# deleting hatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea29d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_type_name(path) -> str:\n",
    "    \"\"\"\n",
    "    Get the path type name safely for any path type.\n",
    "    \n",
    "    Args:\n",
    "        path: Hatch boundary path object\n",
    "        \n",
    "    Returns:\n",
    "        str: Name of the path type\n",
    "    \"\"\"\n",
    "    if hasattr(path, 'path_type'):\n",
    "        path_types = {\n",
    "            1: 'External',\n",
    "            2: 'Polyline',\n",
    "            4: 'Derived',\n",
    "            8: 'Textbox',\n",
    "            16: 'Outermost'\n",
    "        }\n",
    "        return path_types.get(path.path_type, 'Unknown')\n",
    "    return type(path).__name__\n",
    "\n",
    "def analyze_and_remove_hatches(file_path: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Detects, analyzes, and removes hatch entities from a DXF file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the input DXF file\n",
    "        output_path (str): Path to save the output DXF file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = ezdxf.readfile(file_path)\n",
    "        msp = doc.modelspace()\n",
    "        \n",
    "        hatch_entities = msp.query('HATCH')\n",
    "        \n",
    "        for hatch in hatch_entities:\n",
    "            msp.delete_entity(hatch)\n",
    "        \n",
    "        doc.saveas(output_path)\n",
    "        validate_hatches(output_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "def validate_hatches(file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Validates that all hatch entities were properly removed.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the DXF file to validate\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = ezdxf.readfile(file_path)\n",
    "        msp = doc.modelspace()\n",
    "        \n",
    "        remaining_hatches = list(msp.query('HATCH'))\n",
    "        if remaining_hatches:\n",
    "            pass\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "def main4(input_dxf4,output_dxf):\n",
    "    \n",
    "    analyze_and_remove_hatches(input_dxf4, output_dxf)\n",
    "    validate_hatches(output_dxf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f7a88f",
   "metadata": {},
   "source": [
    "# mtext correction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7868d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezdxf\n",
    "from rapidfuzz import process\n",
    "import re\n",
    "\n",
    "# Define the dictionary of correct room names (ordered for priority)\n",
    "room_name_dict = [\n",
    "    'Master Bedroom', 'Bedroom', 'Bathroom', 'Kitchen', 'Living Room', 'Dining Room', 'Foyer', 'Garage', \n",
    "    'Staircase', 'Balcony', 'Reading Area', 'Powder Room', 'Lift', 'Office', 'Hall', 'Storeroom','Parking',\n",
    "    'Utility Area', 'Wash Area', 'Driveway', 'Fence', 'Gate', 'Door', 'Mandir + Dining area',\n",
    "    'Window', 'Wall', 'Ceiling', 'Basement', 'Conference Room', 'Home Theater', 'Pantry Area', \n",
    "    'Garden', 'Entrance', 'Terrace', 'Multipurpose Room', 'Parking Area', 'Store Room', 'Passage', 'Walkway', \n",
    "    'Corridor', 'O.T.S', 'Sitout', 'Terrace Plan','Storage + Wash area','Mandir + Dining area',\n",
    "    'Hanging Garden', 'Window Sitting', 'Vegetable Garden', 'Guest Bedroom', 'Drawing Room','DECK','Walking Area','Meditation Room', \n",
    "    'Atrium', 'Gazebo', 'Dressing Area', 'Mumty', 'WashBasin', 'Lounge', 'V.I.P Lounge', 'Guest Room','Landing Area',\n",
    "    'Parents Bedroom',  'Dining Room', 'Reading Room','Deck','Family Room','Balcony','Garage Door','Garage','Hanging Garden','Hanging Pool',\n",
    "    'Multipurpose Hall'\n",
    "]\n",
    "\n",
    "# Expanded mapping for special cases with broader variations\n",
    "expanded_variations = {\n",
    "    'Bathroom': [\n",
    "                'bathroom', 'bathrom', 'batroom', 'bathrum', 'bathrm', 'bathrooom','Bathroom3','Bathroom 1','Bathroom1','Bathroom2',\n",
    "                'bathroon', 'bathrom', 'bathruom', 'bahtroom', 'bathrm', 'bthroom','Toileeet','Co. Toilet','Bathroom 2','Bathroom4','Bathroom5',\n",
    "                'bathrom', 'bathruum', 'bathrum', 'batheroom','Lavatory', 'bathrome','COMMON TOILET','Bathroom 3^J5','Bathroom 3^J5',\n",
    "                'bath', 'bath_room', 'bath.', 'bath_', 'bthrm', 'bathrm','Existing Toilets','Bathroom 1^J5','Bathroom 2^J5','Bathroom 4^J5'\n",
    "                'rest room', 'toilet', 'washroom', 'loo', 'WC', 'water closet','Toilet\"','{\\FVerdana;\\W3.09116; }TOILET',\n",
    "                'bath-room','TOILET\\P3','TOILET+DRESS','TOILET \\P3','Common\\PToilet\\P8','Common\\PToilet','TOILET 10'\n",
    "            ],\n",
    "            'Bedroom': [\n",
    "                'bedroom','Bedroom1','Bedroom2','Bedroom3', 'bedrom', 'bedrm', 'bedrum', 'beroom', 'bedrom',\n",
    "                'bedroon', 'beadroom', 'bedrooom', 'bedrome','Bedroom4', 'bedrume','BEDROOM\\P',\n",
    "                'bed room', 'bed_room', 'bed.', 'bed_', 'bdrm', 'br',\n",
    "                'bed-room','Bedroo m','BEDROOM\\P 30'\n",
    "            ],\n",
    "            'Kitchen': [\n",
    "                'kitchen', 'kichen', 'kitchn', 'kicthen', 'kithen', 'ktchen',\n",
    "                'kitchon', 'ketchin', 'kitched', 'kitch', 'kitchem', 'kitchine',\n",
    "                'kichen', 'kitchne', 'ktchn', 'ketchn','\\H30x;KITCHEN\\P17',\n",
    "                'kit', 'kit.', 'kit_', 'kitchen_area', 'cooking_area','{\\FVerdana;\\W2.35307; }KITCHEN',\n",
    "                'cook room', 'cookroom', 'ktch', 'ktch.', 'ktn', 'kit-area'\n",
    "            ],\n",
    "            'Living Room': [\n",
    "                'living room', 'livng room', 'living rum', 'liveing room', 'livingroom',\n",
    "                'living rom', 'livin room', 'livving room', 'living roon', 'livroom',\n",
    "                'liv room', 'livingrm', 'living rm','Living Cum Dining',\n",
    "                'lr', 'l_r', 'liv_rm', 'liv.rm', 'living', 'lounge','Living Room',\n",
    "                'living_room', 'living.room', 'liv-room', 'living area'  # Fixed: space, not underscore\n",
    "            ],\n",
    "            'Dining Room': [\n",
    "                'dining room', 'dinning room', 'dining rum', 'dineing room', 'diningroom',\n",
    "                'dining rom', 'dining rm', 'dinin room', 'dinig room', 'dning room',\n",
    "                'dr', 'd_r', 'din_rm', 'din.rm', 'dining', 'dine','Dining Room',\n",
    "                'dining_room', 'dining.room', 'din-room', 'dining area',  # Fixed: space, not underscore\n",
    "                'dinner room', 'dinner_room', 'dining hall', 'dining_hall'\n",
    "            ],\n",
    "            'Foyer': [\n",
    "        'foyer', 'foier', 'foyar', 'foer', 'foyeer', 'foyyer','COMMON FOYER',\n",
    "        'foir', 'foyre', 'foye', 'fayer', 'foerr', 'foyr','COMMON FOYER',\n",
    "        'foeyer', 'foyerr', 'ffoyer', 'foer', 'foyar', 'foyeur',\n",
    "        'foier', 'fioyer', 'fouyer', 'foye', 'foy er', 'fo-yer','Foyer area'\n",
    "    ],\n",
    "            'Garage': [\n",
    "                'garage', 'garag', 'grage', 'garege', 'garaj', 'garrage',\n",
    "                'gabage', 'garege', 'garadge', 'garige', 'garrage', 'garaj',\n",
    "                'grge', 'grage', 'gaage', 'garge',\n",
    "                'gar', 'gar.', 'gar_', 'car_garage', 'auto_garage',\n",
    "                'vehicle storage', 'car storage', 'garage_space', 'grg',\n",
    "                'car garage', 'auto garage', 'car_park', 'car-garage'\n",
    "            ],\n",
    "            'Staircase': [\n",
    "                'staircase', 'stairway', 'stairwell', 'staircase', 'starecase',\n",
    "                'stairces', 'stairkas', 'stairecase', 'staircase', 'stairwel',\n",
    "                'stair', 'str', 'st.', 'stc', 'stair_', 'stairs',\n",
    "                'stair_case', 'stair.case', 'stair-case', 'stairway',\n",
    "                'step', 'steps', 'stair_well', 'stair_way'\n",
    "            ],\n",
    "            'Balcony': [\n",
    "                'balcony', 'balconey', 'balconi', 'balconey', 'balkony',\n",
    "                'balcny', 'balconey', 'balcon', 'balconies', 'balconie',\n",
    "                'bal', 'bal.', 'bal_', 'balc', 'blcny',\n",
    "                'balcony_area', 'open_balcony', \n",
    "                'open_terrace', 'bal-area'\n",
    "            ],\n",
    "            'Powder Room': [\n",
    "        'powder room', 'powderroom', 'powder_room', 'powder-room',\n",
    "        'powdr room', 'powdr_room', 'powderrom', 'powder rom','Powder^JRoom',\n",
    "        'pwd room', 'pwd_room', 'pwdr room', 'pwdrroom','Powder^JRoom',\n",
    "        'powder rm', 'powderrooom', 'powderoom', 'pwder room'\n",
    "            ],\n",
    "\n",
    "            'Lift': [\n",
    "                'elevator', 'elevater', 'elivator', 'elevador', 'elavator',\n",
    "                'elevetor', 'elevtr', 'elevater', 'elivater', 'elevatir',\n",
    "                'elev', 'elv', 'elev.', 'lift', 'lift_area',\n",
    "                'elevator_space', 'lift_core', 'elev_core', 'lift-core',\n",
    "                'elevator_shaft', 'lift_shaft', 'elev-area','{\\FVerdana;\\W3.50148; }LIFT'\n",
    "            ],\n",
    "            'Office': [\n",
    "                'office', 'ofice', 'offise', 'offfice', 'offic',\n",
    "                'ofis', 'offyce', 'ofice', 'oface', 'offis',\n",
    "                'off', 'off.', 'off_', 'workspace', 'work_space',\n",
    "                'office_room', 'work_area', 'office_area', 'off-room',\n",
    "                'working_space', 'work_station', 'off-area'\n",
    "            ],\n",
    "            'Hall': [\n",
    "                'hall', 'hal', 'halll', 'hall1', 'haal', 'hawl', 'haul',\n",
    "                'entry hall', 'entry_hall', 'entry-hall', 'entryhall', 'entr_hall',\n",
    "                'living hall', 'living_hall', 'livinghall', 'liv_hall', 'liv.hall',\n",
    "                'lobby', 'loby', 'lobey', 'lobbey', 'lobb', 'lby',\n",
    "                'entrance hall', 'entrance_hall', 'ent_hall', 'ent.hall',\n",
    "                'hl', 'h_l', 'ent_hl', 'hal_', 'hall_'\n",
    "            ],\n",
    "            'Storeroom': [\n",
    "                'storeroom', 'storroom', 'storoom', 'stor_room', 'store_room',\n",
    "                'storerum', 'storerrom', 'storrm', 'storagerm', 'storge_room',\n",
    "                'storage room', 'storage_room', 'storage-room', 'storage_area',\n",
    "                'store', 'storage', 'strg', 'strg_rm', 'stor_rm', 'str_rm',\n",
    "                'utility room', 'utility_room', 'util_room', 'util_rm',\n",
    "                'cupboard', 'cubboard', 'cupbard', 'cpbrd', 'cup_brd',\n",
    "                'str', 'stor', 'strm', 'st_rm', 'stor_', 'str_','{\\FVerdana;\\W3.61606; }STORE'\n",
    "            ],\n",
    "\n",
    "            'Utility Area': [\n",
    "                'utility room', 'utilty room', 'utility rum', 'util room', 'utlity room',\n",
    "                'utility_room', 'utilityroom', 'util_room', 'utilityrm', 'util_rm',\n",
    "                'laundry room', 'laundry_room', 'laundryroom', 'landry room',\n",
    "                'utility', 'utlty', 'util', 'laundry', 'lundry', 'wash_rm',\n",
    "                'ut_rm', 'ut.rm', 'util', 'ut_', 'util_', 'lndry'\n",
    "            ],\n",
    "            'Wash Area':[\n",
    "                'wash area', 'wash_area', 'washarea', 'washing area', 'washing_area','WASH\\P8',\n",
    "                'WASH \\P8','WASH\\P8','wash_rm','WASH','Wash Area \\P5'\n",
    "            ],\n",
    "\n",
    "            'Pathway': [\n",
    "                'pathway', 'pathwy', 'pathay', 'pthway', 'pathwey', 'pathwai',\n",
    "                'walkway', 'walkwy', 'walk_way', 'walking_path', 'walk path',\n",
    "                'sidewalk', 'side_walk', 'sidewlk', 'side walk', 'sdwlk',\n",
    "                'path', 'pth', 'path_', 'walking_area', 'pedestrian_path',\n",
    "                'pw', 'p_w', 'pthw', 'ww', 'w_w', 'swlk'\n",
    "            ],\n",
    "            'Driveway': [\n",
    "                'driveway', 'drivway', 'drivwey', 'drivwy', 'drivewey', 'drivway',\n",
    "                'car driveway', 'car_driveway', 'cardriveway', 'car_drive',\n",
    "                'parking driveway', 'parking_drive', 'park_drive', 'drive_way',\n",
    "                'vehicle drive', 'vehicle_drive', 'vehicular_path', 'car_path',\n",
    "                'drv', 'dw', 'd_w', 'dr_wy', 'drv_', 'drvwy'\n",
    "            ],\n",
    "            'Fence': [\n",
    "                'fence', 'fense', 'fenc', 'fens', 'fense', 'fench',\n",
    "                'fnc', 'fn', 'f_w', 'bw', 'b_w', 'p_w'\n",
    "            ],\n",
    "            'Gate': [\n",
    "                'gate', 'gat', 'gte', 'geit', 'gaet', 'gatte',\n",
    "                'entrance gate', 'entrance_gate', 'entry gate', 'entry_gate',\n",
    "                'main gate', 'main_gate', 'maingate', 'front_gate',\n",
    "                'exit gate', 'exit_gate', 'side gate', 'side_gate',\n",
    "                'gt', 'g_t', 'ent_gt', 'ext_gt', 'm_gt', 'gat_'\n",
    "            ],\n",
    "            'Door': [\n",
    "                'door', 'dor', 'doar', 'dore', 'doore', 'door1',\n",
    "                'entrance door', 'entrance_door', 'entry door', 'entry_door',\n",
    "                'exit door', 'exit_door', 'front door', 'front_door',\n",
    "                'main door', 'main_door', 'side door', 'side_door',\n",
    "                'dr', 'd_r', 'entr_dr', 'ext_dr', 'm_dr', 'dr_'\n",
    "            ],\n",
    "            'Window': [\n",
    "                'window', 'windo', 'windw', 'wndow', 'windoo', 'winder',\n",
    "                'glass window', 'glass_window', 'glasswindow', 'glass_wind',\n",
    "                'side window', 'side_window', 'sidewindow', 'side_wind',\n",
    "                'ventilation window', 'vent_window', 'vent window', 'vent_wind',\n",
    "                'wnd', 'win', 'w_d', 'gl_wnd', 'v_wnd', 'wndw'\n",
    "            ],\n",
    "            'Wall': [\n",
    "                'wall', 'wal', 'waal', 'woll', 'waall', 'wall1',\n",
    "                'partition wall', 'partition_wall', 'partitionwall', 'part_wall',\n",
    "                'boundary wall', 'boundary_wall', 'boundarywall', 'bound_wall',\n",
    "                'divider', 'divider wall', 'divider_wall', 'div_wall',\n",
    "                'wl', 'w_l', 'p_wl', 'b_wl', 'd_wl', 'wall_'\n",
    "            ],\n",
    "            'Ceiling': [\n",
    "                'ceiling', 'celing', 'cieling', 'ceeling', 'ciling', 'ceilng',\n",
    "                'roof', 'rof', 'ruf', 'roof_ceiling', 'roof_level',\n",
    "                'roofing', 'roofng', 'roof_finish', 'ceiling_finish',\n",
    "                'false ceiling', 'false_ceiling', 'suspended_ceiling', 'susp_ceiling',\n",
    "                'clg', 'c_g', 'rf', 'r_f', 'ceil_', 'roof_'\n",
    "            ],\n",
    "            'Attic': [\n",
    "                'attic', 'atic', 'attik', 'atik', 'attick', 'attics',\n",
    "                'loft', 'lofft', 'lauft', 'loft_space', 'loft_area',\n",
    "                'roof space', 'roof_space', 'roofspace', 'roof_storage',\n",
    "                'attic room', 'attic_room', 'atticroom', 'attic_storage',\n",
    "                'att', 'a_t', 'lft', 'l_f', 'att_', 'loft_'\n",
    "            ],\n",
    "            'Basement': [\n",
    "                'basement', 'basment', 'basemnt', 'basment', 'basement1',\n",
    "                'underground floor', 'underground_floor', 'under_floor',\n",
    "                'cellar', 'celler', 'seller', 'wine_cellar', 'storage_cellar',\n",
    "                'lower floor', 'lower_floor', 'lower_level', 'sub_floor',\n",
    "                'bsmt', 'b_t', 'ug_fl', 'lwr_fl', 'base_', 'cell_'\n",
    "            ],\n",
    "            'Conference Room': [\n",
    "                'conference room', 'conferance room', 'confrence room', 'conf room',\n",
    "                'meeting room', 'meeting_room', 'meetingroom', 'meet_room',\n",
    "                'board room', 'board_room', 'boardroom', 'board_rm',\n",
    "                'conference hall', 'conference_hall', 'conf_hall', 'meeting_hall',\n",
    "                'conf', 'c_r', 'meet_r', 'brd_rm', 'conf_', 'mtg_'\n",
    "            ],\n",
    "            'Home Theater': [\n",
    "                'home theater', 'home theatre', 'hometheater', 'hometheatre',\n",
    "                'media room', 'media_room', 'mediaroom', 'media_center',\n",
    "                'cinema room', 'cinema_room', 'cinemaroom', 'movie_room',\n",
    "                'theater room', 'theatre_room', 'entertainment_room', 'ent_room',\n",
    "                'h_t', 'thtr', 'med_rm', 'cin_rm', 'ent_rm', 'mov_rm'\n",
    "            ],\n",
    "            'Master Bedroom': [\n",
    "                'master bedroom', 'master bedrom', 'master bedrm', 'mastr bedroom',\n",
    "                'master bedrum', 'mastre bedroom', 'master bed room', 'masterbed room',\n",
    "                'master beedroom', 'mstr bedroom', 'master bedroom1', 'masterbedroom',\n",
    "                'master_bedroom', 'master-bedroom', 'master_bed', 'master bed',\n",
    "                'main bedroom', 'main_bedroom', 'main bed', 'main_bed',\n",
    "                'primary bedroom', 'primary_bedroom', 'primary bed', 'primary_bed',\n",
    "                'mstr_bdrm', 'mstr_br', 'mbr', 'm_br', 'mb', 'mbed',\n",
    "                'master_br', 'mstr_bed', 'mstr.bed', 'mstr.br',\n",
    "                'mbdrm', 'mastbr', 'mstrbr', 'mbedroom',\n",
    "                'masterbedroom', 'masterbed', 'masterbr', 'masterbrm',\n",
    "                'mstrbedrm', 'mstrbdrm', 'masterbdrm', 'mstrbed'\n",
    "            ],\n",
    "            'Master Bathroom': [\n",
    "                'master bathroom', 'master bathrom', 'master bathrm', 'mastr bathroom',\n",
    "                'master bathrum', 'mastre bathroom', 'master bath room', 'masterbath room',\n",
    "                'master bathrom', 'mstr bathroom', 'master bathroom1', 'masterbathroom',\n",
    "                'master_bathroom', 'master-bathroom', 'master_bath', 'master bath',\n",
    "                'main bathroom', 'main_bathroom', 'main bath', 'main_bath',\n",
    "                'mstr_bath', 'mstr_ba', 'mba', 'm_ba', 'mbth', 'mbath',\n",
    "                'master_ba', 'mstr_bth', 'mstr.bath', 'mstr.ba',\n",
    "                'mbath', 'mastba', 'mstrba', 'mbathroom','Master Toilet',\n",
    "                'masterbathroom', 'masterbath', 'masterba', 'masterbrm',\n",
    "                'mstrbathrm', 'mstrbath', 'masterbth', 'mstrwc',\n",
    "                'master wc', 'master_wc', 'mstr wc', 'mstr_wc',\n",
    "                'ensuite wc', 'ensuite_wc', 'en suite wc', 'en_suite_wc'\n",
    "            ],\n",
    "            'Pooja Area': [\n",
    "                'pooja room', 'puja room', 'pooja_room', 'puja_room',\n",
    "                'poojaroom', 'pujaroom', 'pooja-room', 'puja-room','Mnndir',\n",
    "                'prayer room', 'prayer_room', 'worship room', 'worship_room',\n",
    "                'mandir room', 'mandir_room', 'temple room', 'temple_room',\n",
    "                'devghar', 'dev_ghar', 'devgriha', 'dev_griha', 'temple',\n",
    "                'meditation room', 'meditation_room', 'spiritual room', 'spiritual_room',\n",
    "                'pooja rum', 'puja rum', 'pooja ruum', 'puja ruum',\n",
    "                'pooja rom', 'puja rom', 'poojha room', 'pujha room',\n",
    "                'pooja rooom', 'puja rooom', 'pooja roon', 'puja roon',\n",
    "                'pr', 'p_r', 'pjr', 'pj_rm', 'p_rm', 'pr_rm',\n",
    "                'pooja_rm', 'puja_rm', 'pooja.rm', 'puja.rm',\n",
    "                'mandir', 'mndr', 'templ', 'tmpl','Mnndir',\n",
    "                'poojamandir', 'pujamandir', 'poojaspace', 'pujaspace',\n",
    "                'poojaarea', 'pujaarea', 'mandirroom', 'templespace',\n",
    "                'devmandir', 'dev_mandir', 'ghar_mandir', 'gharmandir',\n",
    "                'poojamandir', 'puja_mandir', 'home_temple', 'hometemple'\n",
    "            ],\n",
    "            'Garden': [\n",
    "                'garden', 'garde', 'gardn', 'gardenn', 'gaarden',\n",
    "                'gareden', 'gardden', 'grden', 'gardyn', 'gardn',\n",
    "                'garden_area', 'garden_space', 'garden_zone',\n",
    "                'landscaped_area', 'landscape_area', 'landscape_zone',\n",
    "                'green_area', 'green_space', 'green_zone',\n",
    "                'landscape', 'landscaping', 'landscaped',\n",
    "                'planted_area', 'planting_area', 'planter_zone',\n",
    "                'soft_scape', 'softscape', 'soft_area',\n",
    "                'gdn', 'grdn', 'gar', 'gr', 'g',\n",
    "                'gard', 'g_a', 'ga', 'g_z', 'gz',\n",
    "                'lnd', 'lscape', 'lscpe', 'ldsp',\n",
    "                'front_garden', 'rear_garden', 'side_garden','Garden Area',\n",
    "                'back_garden', 'main_garden', 'central_garden',\n",
    "                'private_garden', 'common_garden', 'shared_garden',\n",
    "                'gardenarea', 'gardenspace', 'gardenzone',\n",
    "                'greenarea', 'greenspace', 'greenzone',\n",
    "                'garden1', 'garden2', 'garden3',\n",
    "                'garden_1', 'garden_2', 'garden_3',\n",
    "                'gdn1', 'gdn2', 'gdn3','Garden'\n",
    "            ],\n",
    "            'Entrance': [\n",
    "                'entrance', 'enterance', 'entrence', 'enternce', 'entrans',\n",
    "                'enterence', 'entrense', 'entrace', 'enterence', 'entrenc',\n",
    "                'main_entrance', 'main entrance', 'entry_point', 'entry point',\n",
    "                'front_entrance', 'front entry', 'primary_entrance', 'primary entry',\n",
    "                'side_entrance', 'side entry', 'rear_entrance', 'rear entry',\n",
    "                'entry', 'entry_way', 'entryway', 'entrance_way',\n",
    "                'entrance_lobby', 'entry_lobby', 'entrance_foyer', 'entry_foyer',\n",
    "                'entrance_porch', 'entry_porch', 'entrance_vestibule', 'entry_vestibule',\n",
    "                'ent', 'entr', 'ent_', 'entr_',\n",
    "                'm_ent', 'main_ent', 'f_ent', 'front_ent',\n",
    "                's_ent', 'side_ent', 'r_ent', 'rear_ent',\n",
    "                'main_entry', 'front_entry', 'side_entry', 'back_entry',\n",
    "                'service_entrance', 'service_entry', 'staff_entrance', 'staff_entry',\n",
    "                'visitor_entrance', 'visitor_entry', 'public_entrance', 'public_entry',\n",
    "                'mainentrance', 'frontentrance', 'sideentrance', 'rearentrance',\n",
    "                'mainentry', 'frontentry', 'sideentry', 'rearentry',\n",
    "                'entrypoint', 'entryway', 'entranceway', 'entryarea',\n",
    "                'entrance1', 'entrance2', 'entrance3',\n",
    "                'entry1', 'entry2', 'entry3',\n",
    "                'ent1', 'ent2', 'ent3',\n",
    "                'access', 'access_point', 'ingress', 'ingress_point',\n",
    "                'approach', 'approach_way', 'entry_access', 'entrance_access',\n",
    "                'portal', 'gateway', 'entry_gate', 'entrance_gate',\n",
    "                'doorway', 'main_door', 'front_door', 'entry_door'\n",
    "            ],\n",
    "\n",
    "            'Terrace': [\n",
    "                'terrace', 'Terrace','terace', 'terrase', 'teracce', 'terrase',\n",
    "                'terrasse', 'terasse', 'terace', 'tarrace', 'terrac',\n",
    "                'terrace_area', 'terrace_space', 'terrace_deck','{\\FArial;\\W0.95238; }Trrce',\n",
    "                'open_terrace', 'covered_terrace', 'private_terrace',\n",
    "                'roof_terrace', 'sky_terrace', 'garden_terrace',\n",
    "                'Trrce ','{\\FArial;\\W0.95238; }Trrce',\n",
    "                'patio', 'patio_area', 'outdoor_patio',\n",
    "                'rooftop', 'rooftop_area', \n",
    "                'ter', 'terr', 'tr', 't_r','Trrce',\n",
    "                'ter_', 'terr_', 'tr_', 'trc','{\\FArial;\\W0.95238; }Trrce ',\n",
    "                'ter.', 'terr.', 'tr.', 'trc.',\n",
    "                'front_terrace', 'rear_terrace', 'side_terrace',\n",
    "                'upper_terrace', 'lower_terrace', 'main_terrace',\n",
    "                'private_terrace', 'common_terrace', 'shared_terrace',\n",
    "                'terracearea', 'terracespace', 'terracessit',\n",
    "                'roofterrace', 'skyterrace', 'openterrace',\n",
    "                'deckterrace', 'terraceroom', 'terracespot',\n",
    "                'terrace1', 'terrace2', 'terrace3',\n",
    "                'terrace_1', 'terrace_2', 'terrace_3','Terrace ',\n",
    "                'ter1', 'ter2', 'ter3','{\\FArial;\\W0.95238; }Trrce ',\n",
    "                \n",
    "                'terrace_balcony', 'terrace_patio', 'terrace_lounge',\n",
    "                'outdoor_terrace', 'open_air_terrace', 'top_terrace',\n",
    "                'ter_floor', 'terr_level', 'terrace_level',\n",
    "                'terrace_floor', 'roof_level_terrace', 'top_floor_terrace',\n",
    "                'terrace_garden',  'landscape_terrace'\n",
    "            ],\n",
    "\n",
    "                 'Multipurpose Room': [\n",
    "        # Common spelling variations and misspellings\n",
    "        'multipurpose room', 'multi purpose room', 'multi-purpose room',\n",
    "        'multipurposeroom', 'multi_purpose_room', 'multi.purpose.room',\n",
    "        'multipurpose', 'multi purpose', 'multi-purpose',\n",
    "        'multipurpos room', 'multi purpos room', 'multipurpose rom',\n",
    "        'multipurpos', 'multi purpos', 'multi-purpos',\n",
    "        'multipurpose rm', 'multi purpose rm', 'multi-purpose rm',\n",
    "\n",
    "        # Common abbreviations\n",
    "        'mpr', 'mp_rm', 'mpr_rm', 'm_p_r',\n",
    "        'mp_room', 'm_p_room', 'mp.rm', 'mpr.rm',\n",
    "        'multi_rm', 'mult_rm', 'mprm', 'mpr_',\n",
    "\n",
    "        # Functional variations\n",
    "        'flexible room', 'flexible space', 'flex room',\n",
    "        'flex space', 'flexroom', 'flexspace',\n",
    "        'flexible use room', 'flexible use space',\n",
    "        'adaptable room', 'adaptable space',\n",
    "\n",
    "        # Activity-specific variations\n",
    "        'activity room', 'activity space', 'activity_room',\n",
    "        'function room', 'function hall', 'function_room',\n",
    "        'community room', 'community space', 'community_room',\n",
    "        'common room', 'common space', 'common_room',\n",
    "\n",
    "        # Combined purpose terms\n",
    "        'multi activity room', 'multi_activity_room',\n",
    "        'multi function room', 'multi_function_room',\n",
    "        'multiuse room', 'multi use room', 'multi_use_room',\n",
    "        'all purpose room', 'all-purpose room', 'all_purpose_room',\n",
    "\n",
    "        # Technical variations\n",
    "        'versatile space', 'versatile room', 'versatile_room',\n",
    "        'convertible space', 'convertible room', 'convertible_room',\n",
    "        'utility space', 'utility room', 'utility_room',\n",
    "\n",
    "        # With numbers\n",
    "        'multipurpose room1', 'multipurpose room 1', 'multipurpose_room_1',\n",
    "        'mpr1', 'mpr 1', 'mpr_1',\n",
    "        'multi purpose 1', 'multi-purpose 1', 'multi_purpose_1',\n",
    "\n",
    "        # Additional descriptive variations\n",
    "        'mixed use room', 'mixed-use room', 'mixed_use_room',\n",
    "        'shared space room', 'shared-space room', 'shared_space_room',\n",
    "        'general purpose room', 'general-purpose room', 'general_purpose_room',\n",
    "\n",
    "        # Professional/technical terms\n",
    "        'multifunctional room', 'multi functional room', 'multi_functional_room',\n",
    "        'multimodal space', 'multi modal space', 'multi_modal_space',\n",
    "        'polyvalent room', 'poly valent room', 'poly_valent_room',\n",
    "\n",
    "        # Special use cases\n",
    "        'events room', 'event room', 'event_room',\n",
    "        'program room', 'programme room', 'program_room',\n",
    "        'activities room', 'activities space', 'activities_room',\n",
    "\n",
    "        # Combined abbreviations\n",
    "        'mpurp_rm', 'mpurp.rm', 'm_purp_rm',\n",
    "        'mult_purp', 'mlt_prp', 'mp_space',\n",
    "        'flex_rm', 'flex.rm', 'flex_space'\n",
    "    ],\n",
    "\n",
    "            'Store Room': [\n",
    "                'store room', 'storage room', 'store', 'storage', 'utility room',\n",
    "                'storage area', 'store area', 'utility storage', 'storage closet',\n",
    "                'storage space', 'storeroom','STORE\\P 7','STORE','STORE\\P','STORE'\n",
    "            ],\n",
    "\n",
    "        'Passage': [\n",
    "            # Common spelling variations and misspellings\n",
    "            'passage', 'passge', 'passag', 'pasage', 'passg',\n",
    "            'passege', 'passaje', 'passege', 'passige', 'pasege',\n",
    "            'passageway', 'passage way', 'passage-way',\n",
    "\n",
    "            # Common abbreviations\n",
    "            'psg', 'pas', 'psg_', 'psgw', 'psge',\n",
    "            'pass', 'passe', 'p_way', 'p.way', 'p_w',\n",
    "            'psg.', 'pas.', 'pass.', 'p.', 'pw.',\n",
    "\n",
    "            # Location-specific variations\n",
    "            'front passage', 'front_passage', 'front-passage',\n",
    "            'rear passage', 'rear_passage', 'rear-passage',\n",
    "            'side passage', 'side_passage', 'side-passage',\n",
    "            'main passage', 'main_passage', 'main-passage',\n",
    "            'service passage', 'service_passage', 'service-passage',\n",
    "\n",
    "            # Functional variations\n",
    "            'connecting passage', 'connection passage', 'connector passage',\n",
    "            'linking passage', 'link passage', 'connecting_passage',\n",
    "            'circulation passage', 'circu passage', 'circ_passage',\n",
    "            'access passage', 'access_passage', 'access-passage',\n",
    "            'movement passage', 'movement_passage', 'move_passage',\n",
    "\n",
    "            # With numbers for identification\n",
    "            'passage1', 'passage2', 'passage3',\n",
    "            'passage_1', 'passage_2', 'passage_3',\n",
    "            'psg1', 'psg2', 'psg3','Wide Service Passage'\n",
    "\n",
    "            # Technical variations\n",
    "            'thoroughfare', 'thorough_fare', 'thorough-fare',\n",
    "            'walkthrough', 'walk_through', 'walk-through',\n",
    "            'walkway', 'walk_way', 'walk-way',\n",
    "\n",
    "            # Combined terms\n",
    "            'passage corridor', 'passage_corridor', 'passage-corridor',\n",
    "            'passage hall', 'passage_hall', 'passage-hall',\n",
    "            'passage area', 'passage_area', 'passage-area',\n",
    "\n",
    "            # Usage-specific\n",
    "            'entry passage', 'entry_passage', 'entry-passage',\n",
    "            'exit passage', 'exit_passage', 'exit-passage',\n",
    "            'through passage', 'through_passage', 'thru_passage'\n",
    "        ],\n",
    "            'Walkway':[\n",
    "\n",
    "            # Technical variations\n",
    "            'thoroughfare', 'thorough_fare', 'thorough-fare',\n",
    "            'walkthrough', 'walk_through', 'walk-through',\n",
    "            'walkway', 'walk_way', 'walk-way','Walkway'\n",
    "            ],\n",
    "\n",
    "        'Corridor': [\n",
    "            # Common spelling variations and misspellings\n",
    "            'corridor', 'corridr', 'corridore', 'coridoor', 'corridoor',\n",
    "            'corador', 'coridor', 'corridr', 'coridore', 'corridore',\n",
    "            'coridoor', 'corridoor', 'corrdr', 'corridr', 'corridoor',\n",
    "\n",
    "            # Common abbreviations\n",
    "            'cor', 'corr', 'crdr', 'crd', 'cdr',\n",
    "            'cor.', 'corr.', 'c.', 'cr.', 'crd.',\n",
    "            'cor_', 'corr_', 'c_', 'cr_', 'crd_',\n",
    "\n",
    "            # Location-specific variations\n",
    "            'main corridor', 'main_corridor', 'main-corridor',\n",
    "            'side corridor', 'side_corridor', 'side-corridor',\n",
    "            'central corridor', 'central_corridor', 'central-corridor',\n",
    "            'inner corridor', 'inner_corridor', 'inner-corridor',\n",
    "            'outer corridor', 'outer_corridor', 'outer-corridor',\n",
    "\n",
    "            # Functional variations\n",
    "            'service corridor', 'service_corridor', 'service-corridor',\n",
    "            'public corridor', 'public_corridor', 'public-corridor',\n",
    "            'private corridor', 'private_corridor', 'private-corridor',\n",
    "            'access corridor', 'access_corridor', 'access-corridor',\n",
    "            'circulation corridor', 'circulation_corridor', 'circ_corridor',\n",
    "\n",
    "            # With numbers for identification\n",
    "            'corridor1', 'corridor2', 'corridor3',\n",
    "            'corridor_1', 'corridor_2', 'corridor_3',\n",
    "            'cor1', 'cor2', 'cor3',\n",
    "            'corr1', 'corr2', 'corr3',\n",
    "\n",
    "            # Floor-specific\n",
    "            'ground corridor', 'ground_corridor', 'ground-corridor',\n",
    "            'first corridor', 'first_corridor', 'first-corridor',\n",
    "            'upper corridor', 'upper_corridor', 'upper-corridor',\n",
    "\n",
    "            # Technical variations\n",
    "            'hallway', 'hall_way', 'hall-way',\n",
    "            'passageway', 'passage_way', 'passage-way',\n",
    "            'circulation path', 'circulation_path', 'circ_path'\n",
    "\n",
    "            # Usage-specific\n",
    "            'connecting corridor', 'connecting_corridor', 'connect-corridor',\n",
    "            'link corridor', 'link_corridor', 'link-corridor',\n",
    "            'transition corridor', 'transition_corridor', 'trans-corridor',\n",
    "\n",
    "            # Combined terms\n",
    "            'corridor passage', 'corridor_passage', 'corridor-passage',\n",
    "            'corridor hall', 'corridor_hall', 'corridor-hall',\n",
    "            'corridor space', 'corridor_space', 'corridor-space',\n",
    "\n",
    "            # Special purpose\n",
    "            'fire corridor', 'fire_corridor', 'fire-corridor',\n",
    "            'escape corridor', 'escape_corridor', 'escape-corridor',\n",
    "            'emergency corridor', 'emergency_corridor', 'emerg-corridor',\n",
    "\n",
    "            # Alternative terms\n",
    "            'gallery', 'galleria', 'walkway',\n",
    "            'concourse', 'arcade', 'promenade',\n",
    "            'aisle', 'passageway', 'pathway'\n",
    "        ],\n",
    "            'Powder Room': [\n",
    "        'powder room', 'powderroom', 'powder_room', 'powder-room',\n",
    "        'powdr room', 'powdr_room', 'powderrom', 'powder rom','Powder^JRoom',\n",
    "        'pwd room', 'pwd_room', 'pwdr room', 'pwdrroom','Powder^JRoom',\n",
    "        'powder rm', 'powderrooom', 'powderoom', 'pwder room'\n",
    "            ],\n",
    "            'Multipurpose Room': [\n",
    "        # Common spelling variations and misspellings\n",
    "        'multipurpose room', 'multi purpose room', 'multi-purpose room',\n",
    "        'multipurposeroom', 'multi_purpose_room', 'multi.purpose.room',\n",
    "        'multipurpose', 'multi purpose', 'multi-purpose',\n",
    "        'multipurpos room', 'multi purpos room', 'multipurpose rom',\n",
    "        'multipurpos', 'multi purpos', 'multi-purpos',\n",
    "        'multipurpose rm', 'multi purpose rm', 'multi-purpose rm',\n",
    "\n",
    "        # Common abbreviations\n",
    "        'mpr', 'mp_rm', 'mpr_rm', 'm_p_r',\n",
    "        'mp_room', 'm_p_room', 'mp.rm', 'mpr.rm',\n",
    "        'multi_rm', 'mult_rm', 'mprm', 'mpr_',\n",
    "\n",
    "        # Functional variations\n",
    "        'flexible room', 'flexible space', 'flex room',\n",
    "        'flex space', 'flexroom', 'flexspace',\n",
    "        'flexible use room', 'flexible use space',\n",
    "        'adaptable room', 'adaptable space',\n",
    "\n",
    "        # Activity-specific variations\n",
    "        'activity room', 'activity space', 'activity_room',\n",
    "        'function room', 'function hall', 'function_room',\n",
    "        'community room', 'community space', 'community_room',\n",
    "        'common room', 'common space', 'common_room',\n",
    "\n",
    "        # Combined purpose terms\n",
    "        'multi activity room', 'multi_activity_room',\n",
    "        'multi function room', 'multi_function_room',\n",
    "        'multiuse room', 'multi use room', 'multi_use_room',\n",
    "        'all purpose room', 'all-purpose room', 'all_purpose_room',\n",
    "\n",
    "        # Technical variations\n",
    "        'versatile space', 'versatile room', 'versatile_room',\n",
    "        'convertible space', 'convertible room', 'convertible_room',\n",
    "        'utility space', 'utility room', 'utility_room',\n",
    "\n",
    "        # With numbers\n",
    "        'multipurpose room1', 'multipurpose room 1', 'multipurpose_room_1',\n",
    "        'mpr1', 'mpr 1', 'mpr_1',\n",
    "        'multi purpose 1', 'multi-purpose 1', 'multi_purpose_1',\n",
    "\n",
    "        # Additional descriptive variations\n",
    "        'mixed use room', 'mixed-use room', 'mixed_use_room',\n",
    "        'shared space room', 'shared-space room', 'shared_space_room',\n",
    "        'general purpose room', 'general-purpose room', 'general_purpose_room',\n",
    "\n",
    "        # Professional/technical terms\n",
    "        'multifunctional room', 'multi functional room', 'multi_functional_room',\n",
    "        'multimodal space', 'multi modal space', 'multi_modal_space',\n",
    "        'polyvalent room', 'poly valent room', 'poly_valent_room',\n",
    "\n",
    "        # Special use cases\n",
    "        'events room', 'event room', 'event_room',\n",
    "        'program room', 'programme room', 'program_room',\n",
    "        'activities room', 'activities space', 'activities_room',\n",
    "\n",
    "        # Combined abbreviations\n",
    "        'mpurp_rm', 'mpurp.rm', 'm_purp_rm',\n",
    "        'mult_purp', 'mlt_prp', 'mp_space',\n",
    "        'flex_rm', 'flex.rm', 'flex_space'\n",
    "    ],\n",
    "'O.T.S': [\n",
    "        'ots', 'OTS', 'O.T.S', 'O.T.S.', 'OTS1', 'OTS 1', 'O.T.S 1', 'O.T.S. 1', 'OTS2', 'OTS 2','OTs','ots' \n",
    "        'O.T.S 2', 'O.T.S. 2', 'OTS3', 'OTS 3', 'O.T.S 3', 'O.T.S. 3', 'O T S', 'OT', 'OT.S','OTS ' ,'OTS',\n",
    "        'Open To Sky', 'open to sky', 'Open-To-Sky', 'open-to-sky', 'OpToSky', 'O.T.Sky', 'OTSky',\n",
    "        'Open_To_Sky', 'open_to_sky', 'OTS area', 'O.T.S. area', 'O.T.S area', 'Sky area', 'otsa',\n",
    "    ],\n",
    "    \n",
    "            'Passage': [\n",
    "            # Common spelling variations and misspellings\n",
    "            'passage', 'passge', 'passag', 'pasage', 'passg',\n",
    "            'passege', 'passaje', 'passege', 'passige', 'pasege',\n",
    "            'passageway', 'passage way', 'passage-way',\n",
    "\n",
    "            # Common abbreviations\n",
    "            'psg', 'pas', 'psg_', 'psgw', 'psge',\n",
    "            'pass', 'passe', 'p_way', 'p.way', 'p_w',\n",
    "            'psg.', 'pas.', 'pass.', 'p.', 'pw.',\n",
    "\n",
    "            # Location-specific variations\n",
    "            'front passage', 'front_passage', 'front-passage',\n",
    "            'rear passage', 'rear_passage', 'rear-passage',\n",
    "            'side passage', 'side_passage', 'side-passage',\n",
    "            'main passage', 'main_passage', 'main-passage',\n",
    "            'service passage', 'service_passage', 'service-passage',\n",
    "\n",
    "            # Functional variations\n",
    "            'connecting passage', 'connection passage', 'connector passage',\n",
    "            'linking passage', 'link passage', 'connecting_passage',\n",
    "            'circulation passage', 'circu passage', 'circ_passage',\n",
    "            'access passage', 'access_passage', 'access-passage',\n",
    "            'movement passage', 'movement_passage', 'move_passage',\n",
    "\n",
    "            # With numbers for identification\n",
    "            'passage1', 'passage2', 'passage3',\n",
    "            'passage_1', 'passage_2', 'passage_3',\n",
    "            'psg1', 'psg2', 'psg3','Wide Service Passage'\n",
    "\n",
    "            # Technical variations\n",
    "            'thoroughfare', 'thorough_fare', 'thorough-fare',\n",
    "            'walkthrough', 'walk_through', 'walk-through',\n",
    "            'walkway', 'walk_way', 'walk-way',\n",
    "\n",
    "            # Combined terms\n",
    "            'passage corridor', 'passage_corridor', 'passage-corridor',\n",
    "            'passage hall', 'passage_hall', 'passage-hall',\n",
    "            'passage area', 'passage_area', 'passage-area','Walking Area',\n",
    "\n",
    "            # Usage-specific\n",
    "            'entry passage', 'entry_passage', 'entry-passage',\n",
    "            'exit passage', 'exit_passage', 'exit-passage',\n",
    "            'through passage', 'through_passage', 'thru_passage'\n",
    "        ],\n",
    "            'Sitout': [\n",
    "        # Common spelling variations and misspellings\n",
    "        'sitout', 'sit out', 'sit-out', 'sit_out',\n",
    "        'sittout', 'site out', 'siteout', 'site-out',\n",
    "        'sitoot', 'sittingout', 'sitting out', 'sitting-out',\n",
    "        'sitdown', 'sit down', 'sit-down', 'sit_down',\n",
    "\n",
    "        # Common abbreviations\n",
    "        'so', 's_o', 'st', 'st_o',\n",
    "        'sto', 'st.o', 'st.out', 'st_out',\n",
    "        'sout', 's.out', 's_out', 'st.o.',\n",
    "\n",
    "\n",
    "        # Location specific\n",
    "        'front sitout', 'front_sitout', 'front-sitout',\n",
    "        'rear sitout', 'rear_sitout', 'rear-sitout',\n",
    "        'side sitout', 'side_sitout', 'side-sitout',\n",
    "        'main sitout', 'main_sitout', 'main-sitout',\n",
    "\n",
    "\n",
    "\n",
    "        # With numbers for identification\n",
    "        'sitout1', 'sitout2', 'sitout3',\n",
    "        'sitout_1', 'sitout_2', 'sitout_3',\n",
    "        'sit out 1', 'sit out 2', 'sit out 3',\n",
    "\n",
    "        # Combined terms\n",
    "        'sitout terrace', 'sitout_terrace', 'sitout-terrace',\n",
    "        'sitout deck', 'sitout_deck', 'sitout-deck',\n",
    "        'sitout area', 'sitout_area', 'sitout-area',\n",
    "\n",
    "\n",
    "    ],\n",
    "    'Deck': [\n",
    "    # Common spelling variations and misspellings\n",
    "    'deck', 'dec', 'dek', 'decks',\n",
    "    'dek', 'dack', 'dekk', 'dock',\n",
    "    'dek room', 'deck room', 'deck-room', 'deck_room',\n",
    "    'dek area', 'deck area', 'deck-area', 'deck_area',\n",
    "    'dck', 'decc', 'dech', 'deckk','DECK',\n",
    "    'open deck', 'open-deck', 'open_deck',\n",
    "    'wooden deck', 'wood deck', 'wood-deck'\n",
    "    ],\n",
    "\n",
    "        'Corridor': [\n",
    "            # Common spelling variations and misspellings\n",
    "            'corridor', 'corridr', 'corridore', 'coridoor', 'corridoor',\n",
    "            'corador', 'coridor', 'corridr', 'coridore', 'corridore',\n",
    "            'coridoor', 'corridoor', 'corrdr', 'corridr', 'corridoor',\n",
    "\n",
    "            # Common abbreviations\n",
    "            'cor', 'corr', 'crdr', 'crd', 'cdr',\n",
    "            'cor.', 'corr.', 'c.', 'cr.', 'crd.',\n",
    "            'cor_', 'corr_', 'c_', 'cr_', 'crd_',\n",
    "\n",
    "            # Location-specific variations\n",
    "            'main corridor', 'main_corridor', 'main-corridor',\n",
    "            'side corridor', 'side_corridor', 'side-corridor',\n",
    "            'central corridor', 'central_corridor', 'central-corridor',\n",
    "            'inner corridor', 'inner_corridor', 'inner-corridor',\n",
    "            'outer corridor', 'outer_corridor', 'outer-corridor',\n",
    "\n",
    "            # Functional variations\n",
    "            'service corridor', 'service_corridor', 'service-corridor',\n",
    "            'public corridor', 'public_corridor', 'public-corridor',\n",
    "            'private corridor', 'private_corridor', 'private-corridor',\n",
    "            'access corridor', 'access_corridor', 'access-corridor',\n",
    "            'circulation corridor', 'circulation_corridor', 'circ_corridor',\n",
    "\n",
    "            # With numbers for identification\n",
    "            'corridor1', 'corridor2', 'corridor3',\n",
    "            'corridor_1', 'corridor_2', 'corridor_3',\n",
    "            'cor1', 'cor2', 'cor3',\n",
    "            'corr1', 'corr2', 'corr3',\n",
    "\n",
    "            # Floor-specific\n",
    "            'ground corridor', 'ground_corridor', 'ground-corridor',\n",
    "            'first corridor', 'first_corridor', 'first-corridor',\n",
    "            'upper corridor', 'upper_corridor', 'upper-corridor',\n",
    "\n",
    "            # Technical variations\n",
    "            'hallway', 'hall_way', 'hall-way',\n",
    "            'passageway', 'passage_way', 'passage-way',\n",
    "            'circulation path', 'circulation_path', 'circ_path'\n",
    "\n",
    "            # Usage-specific\n",
    "            'connecting corridor', 'connecting_corridor', 'connect-corridor',\n",
    "            'link corridor', 'link_corridor', 'link-corridor',\n",
    "            'transition corridor', 'transition_corridor', 'trans-corridor',\n",
    "\n",
    "            # Combined terms\n",
    "            'corridor passage', 'corridor_passage', 'corridor-passage',\n",
    "            'corridor hall', 'corridor_hall', 'corridor-hall',\n",
    "            'corridor space', 'corridor_space', 'corridor-space',\n",
    "\n",
    "            # Special purpose\n",
    "            'fire corridor', 'fire_corridor', 'fire-corridor',\n",
    "            'escape corridor', 'escape_corridor', 'escape-corridor',\n",
    "            'emergency corridor', 'emergency_corridor', 'emerg-corridor',\n",
    "\n",
    "            # Alternative terms\n",
    "            'gallery', 'galleria', 'walkway',\n",
    "            'concourse', 'arcade', 'promenade',\n",
    "            'aisle', 'passageway', 'pathway'\n",
    "        ],\n",
    "            'Ground Floor Plan': [\n",
    "                'ground floor plan', 'ground floor', 'gf plan', 'g/f plan',\n",
    "                'ground level plan', 'ground level', 'ground floor layout',\n",
    "                'gf layout', 'ground floor drawing', 'ground plan', 'g.f plan',\n",
    "                'gf', 'g/f', 'g.f','{\\FVerdana;\\W5.26240; }GROUND FLOOR'\n",
    "            ],\n",
    "            'First Floor Plan': [\n",
    "                'first floor plan', 'first floor', 'ff plan', 'f/f plan',\n",
    "                'first level plan', 'first level', 'first floor layout',\n",
    "                'ff layout', 'first floor drawing', 'ff', 'f/f', 'f.f',\n",
    "                'f.f plan', '1st floor plan', '1st floor'\n",
    "            ],\n",
    "            'Atrium': [\n",
    "        'atrium', 'courtyard', 'central hall', 'interior court', 'lobby',\n",
    "        'open space', 'glass hall', 'entrance hall', 'reception area',\n",
    "        'central garden', 'indoor plaza', 'light well', 'skylit court',\n",
    "         'indoor courtyard', 'winter garden', 'pavilion',\n",
    "        'grand entrance', 'rotunda', 'central space','atrium', 'atrum', 'attrium', 'atreum', 'atruim', 'atriam', \n",
    "        'atriam', 'atryum', 'atrim', 'atriam', 'artium', 'atriom', \n",
    "        'atriumm', 'aatrium', 'atriun', 'atriun', 'atrimu', 'atrrium',\n",
    "        'atriuum', 'atreum', 'atrui', 'atrim', 'a-trium', 'at rium'\n",
    "    ],\n",
    "\n",
    "            \"Gazebo\": [\n",
    "        \"gazebo\",\"garden gazebo\",\"outdoor pavilion\",\"garden pavilion\",\"garden shelter\",\"pergola\",\"outdoor shelter\",\"patio gazebo\",\"gazebo structure\",\"garden house\",\"summer house\",\"outdoor sitting area\",\"covered patio structure\",\"backyard gazebo\",\"garden retreat\"\n",
    "    ],\n",
    "\n",
    "\n",
    "            'Dressing Area': [\n",
    "                'dressing area', 'dressing room', 'dressing', 'dress room','Wardrobe','Walk-In\\PCloset ','Walk-in\\PCloset ','Walk-in\\PCloset',\n",
    "                'wardrobe room', 'closet room', 'walk-in closet', 'walk in closet','Walk-in\\PCloset','Dressing Area 1','Dressing Area 2',\n",
    "                'Dressing Area 3','Dressing Area1','Dressing Area2','Dressing Area3','Dressing Area4','Dressing 1','Dressing 2','Dressing 3',\n",
    "                'dressing space', 'changing room', 'wardrobe area', 'dressing chamber','Wardrobe Pathway',\n",
    "            ],\n",
    "            'Mumty': [\n",
    "        'mumty', 'roof turret', 'roof lantern', 'cupola', 'roof monitor',\n",
    "        'roof belvedere', 'roof pavilion', 'roof house', 'roof structure',\n",
    "        'dormer structure', 'roof tower', 'roof observatory', 'lookout turret',\n",
    "        'roof extension', 'attic structure', 'roof projection', 'roof addition',\n",
    "                'mumty', 'mmty', 'muty', 'mummty', 'mumtty', 'mumpty', \n",
    "        'mumti', 'munty', 'momty', 'mumdy', 'mamty', 'mumyt', '{\\FArial;\\W0.80952; }Mmty',\n",
    "        'mtmy', 'mutmy', 'numty', 'mum-ty', 'mum ty', 'mumtey',\n",
    "        'mumyy', 'mumtiy', 'mumty1', 'mummtty', 'mumtyy','Mumty\\P'\n",
    "    ],\n",
    "            'WashBasin': [\n",
    "        'wash basin', 'washbasin', 'wash bassin', 'wash-basin','Basin', 'washbowl',\n",
    "        'wash basiin', 'wash basn', 'wash bason', 'wash basen', 'wash baisin',\n",
    "        'wash basin', 'wash basic', 'washbasin', 'washbaisin', 'wash bazin','Wash^JBasin',\n",
    "        'wash bashin', 'wash basin', 'wash-bowl', 'was basin', 'wash bason',\n",
    "        'wash bossin', 'wash baisn','WashBasin', 'whash basin', 'wash bisin', 'wahsbasin'\n",
    "    ],\n",
    "            'Lounge': [\n",
    "        # Common spelling variations and misspellings\n",
    "        'lounge', 'loung', 'lounj', 'loungue', 'louneg', 'lunge', 'loungee',\n",
    "        'longue', 'launje', 'laungh', 'loungh', 'launge', 'launce', 'loung area','\\pxi-3,l4,t4;T.V.^ILounge',\n",
    "        'launge', 'longue', 'lunj', 'loung e', 'loung-e', 'lougne', 'loungge','ILounge','\\pxi-3,l4,t4;T.V.^ILounge'\n",
    "\n",
    "        # Common abbreviations\n",
    "        'lng', 'lnge', 'lge', 'loun.', 'lou.', 'l.area', 'loun_area'\n",
    "            ],\n",
    "            'V.I.P lounge': [\n",
    "        'V.I.P lounge', 'VIP lounge', 'exclusive lounge', 'premium lounge',\n",
    "        'private lounge', 'elite area', 'members lounge', 'executive lounge',\n",
    "        'priority lounge', 'first-class lounge', 'luxury waiting area',\n",
    "        'preferred guest area', 'celebrity lounge', 'high-profile guest area',\n",
    "        'privileged access lounge', 'special guest lounge', 'premier lounge',\n",
    "        'reserved lounge', 'select guest area', 'concierge lounge','V.I.P lounge', 'VIP lounge', 'V.I.P. lounge', 'vip lounge',\n",
    "        'V.I.P loung', 'V.I.P. longue', 'V.IP lounge', 'V.I.P launge',\n",
    "        'V.I.P longe', 'V.I.P loungue', 'VIP longue', 'V.I.P lounj',\n",
    "        'VI.P lounge', 'V.I.P lounger', 'V.I.P lunge', 'VIP lownge',\n",
    "        'V.I.P. launch', 'V.I.P louge', 'V I P lounge', 'vip-lounge'\n",
    "    ],\n",
    "            'Guest Room': [\n",
    "                'guest room', 'guest bedroom', 'guest suite', 'visitors room',\n",
    "                'guest quarters', 'guest chamber', 'guest accommodation',\n",
    "                'guest sleeping room', 'visitor bedroom', 'visitor suite',\n",
    "                'guest bed room', 'guest living space'\n",
    "            ]\n",
    "}\n",
    "\n",
    "# Mapping for specific direct translations\n",
    "specific_mappings = {\n",
    "    'PUJA ROOM': 'Pooja Area/Pooja Room',\n",
    "    'DINNING AREA': 'Dining Room',\n",
    "    'DINNING ROOM': 'Dining Room',\n",
    "    'PARENTS BEDROOM': 'Parents Bedroom',\n",
    "    'READING ROOM': 'Reading Area',\n",
    "    'MULTIPURPOSE HALL': 'Multipurpose Room',\n",
    "    'SERVICE PASSAGE': 'Passage',\n",
    "    'LIFT': 'Elevator',\n",
    "    'Dressing Area':'Wardrobe Pathway',\n",
    "    'Dressing Area':'Dressing 1',\n",
    "    'Family Area':'Family Room'\n",
    "}\n",
    "\n",
    "def split_at_format_marker(text):\n",
    "    \"\"\"Split text at formatting codes and return both parts.\"\"\"\n",
    "    # Remove \\pxi format codes from the text\n",
    "    text = re.sub(r'\\\\pxi[^;]*;', '', text)\n",
    "    \n",
    "    # Common format markers in AutoCAD\n",
    "    markers = ['\\\\P', '\\\\p', '^j', '\\\\H','^J']\n",
    "    \n",
    "    # Find position of first marker\n",
    "    positions = []\n",
    "    for marker in markers:\n",
    "        pos = text.find(marker)\n",
    "        if pos != -1:\n",
    "            positions.append((pos, marker))\n",
    "    \n",
    "    if positions:\n",
    "        # Get the earliest marker\n",
    "        pos, marker = min(positions, key=lambda x: x[0])\n",
    "        return text[:pos].strip(), marker + text[pos+len(marker):]\n",
    "    \n",
    "    # No marker found\n",
    "    return text.strip(), \"\"\n",
    "\n",
    "def extract_room_info(text):\n",
    "    \"\"\"Extract the room name and dimensions from text like 'Living room 1\\P17' X 14''\"\"\"\n",
    "    # Split by format markers\n",
    "    room_part, format_part = split_at_format_marker(text)\n",
    "    \n",
    "    # Extract dimensions if they exist in the format part\n",
    "    dimensions = \"\"\n",
    "    if format_part:\n",
    "        # Look for dimension patterns like \"17' X 14'\" in the format part\n",
    "        dim_match = re.search(r'(\\d+[\\'\"]?\\s*[Xx]\\s*\\d+[\\'\"]?)', format_part)\n",
    "        if dim_match:\n",
    "            dimensions = dim_match.group(1)\n",
    "    \n",
    "    return room_part.strip(), dimensions, format_part\n",
    "\n",
    "def correct_text(text):\n",
    "    \"\"\"Corrects the text based on expanded variations first, then room_name_dict using fuzzy matching.\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return text\n",
    "    \n",
    "    # Remove any \\pxi format codes\n",
    "    text = re.sub(r'\\\\pxi[^;]*;', '', text)\n",
    "    \n",
    "    # Handle format markers\n",
    "    main_text, format_part = split_at_format_marker(text)\n",
    "    corrected_room = correct_room_name(main_text)\n",
    "    \n",
    "    return corrected_room + format_part\n",
    "\n",
    "def correct_room_name(room_name):\n",
    "    \"\"\"Core function to correct just the room name part.\"\"\"\n",
    "    if not room_name or not room_name.strip():\n",
    "        return room_name\n",
    "    \n",
    "    # Check expanded variation dictionaries first (more flexible matching)\n",
    "    room_upper = room_name.upper()\n",
    "    for correct_name, variations in expanded_variations.items():\n",
    "        for variation in variations:\n",
    "            if room_upper == variation.upper() or room_name == variation:\n",
    "                return correct_name.upper()\n",
    "    \n",
    "    # Check for specific mappings (case insensitive)\n",
    "    for key, value in specific_mappings.items():\n",
    "        if room_upper == key:\n",
    "            return value.upper()\n",
    "    \n",
    "    # Convert to title case for matching\n",
    "    room_title = room_name.strip().title()\n",
    "    \n",
    "    # Try fuzzy matching with the original dictionary\n",
    "    result = process.extractOne(room_title, room_name_dict, score_cutoff=85)\n",
    "    \n",
    "    if result:\n",
    "        return result[0].upper()\n",
    "    else:\n",
    "        # If no match found, just return the uppercase version\n",
    "        return room_title.upper()\n",
    "\n",
    "def correct_dxf_text(file_path, output_path):\n",
    "    \"\"\"Corrects TEXT and MTEXT entities in a DXF file.\"\"\"\n",
    "    doc = ezdxf.readfile(file_path)  # Load DXF file\n",
    "    msp = doc.modelspace()  # Access model space\n",
    "    \n",
    "    corrections_made = 0\n",
    "    unchanged_count = 0\n",
    "    \n",
    "    # Process TEXT entities\n",
    "    for text_entity in msp.query(\"TEXT\"):\n",
    "        old_text = text_entity.dxf.text\n",
    "        new_text = correct_text(old_text)\n",
    "        \n",
    "        if old_text != new_text:  # Only update if correction is made\n",
    "            print(f\"Corrected TEXT: '{old_text}'  '{new_text}'\")\n",
    "            text_entity.dxf.text = new_text\n",
    "            corrections_made += 1\n",
    "        else:\n",
    "            unchanged_count += 1\n",
    "    \n",
    "    # Process MTEXT entities\n",
    "    for mtext_entity in msp.query(\"MTEXT\"):\n",
    "        old_text = mtext_entity.dxf.text\n",
    "        new_text = correct_text(old_text)\n",
    "        \n",
    "        if old_text != new_text:  # Only update if correction is made\n",
    "            print(f\"Corrected MTEXT: '{old_text}'  '{new_text}'\")\n",
    "            mtext_entity.dxf.text = new_text\n",
    "            corrections_made += 1\n",
    "        else:\n",
    "            unchanged_count += 1\n",
    "    \n",
    "    # Save the corrected DXF file\n",
    "    doc.saveas(output_path)\n",
    "    \n",
    "    # Display summary\n",
    "    #print(f\"\\nSummary:\")\n",
    "    #print(f\"Total text entities: {corrections_made + unchanged_count}\")\n",
    "    #print(f\"Corrected entities: {corrections_made}\")\n",
    "    #print(f\"Unchanged entities: {unchanged_count}\")\n",
    "    #print(f\"Corrected DXF saved as: {output_path}\")\n",
    "    \n",
    "\n",
    "def load_dictionary():\n",
    "    # Return the existing dictionaries as a dictionary\n",
    "    return {\n",
    "        'room_name_dict': room_name_dict,\n",
    "        'expanded_variations': expanded_variations,\n",
    "        'specific_mappings': specific_mappings\n",
    "    }\n",
    "\n",
    "def main5(input_dxf5, output_dxf):\n",
    "    dictionary = load_dictionary()\n",
    "    correct_dxf_text(input_dxf5, output_dxf)\n",
    "    #print(f\"Corrected DXF saved as {output_dxf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a1744",
   "metadata": {},
   "source": [
    "# floor distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "364eca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ezdxf\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def slice_text_entity(text):\n",
    "    \"\"\"\n",
    "    Parse text entity into name, dimensions, and suffix parts.\n",
    "    Handles both regular text and multi-line text with \\P separator.\n",
    "    \"\"\"\n",
    "    text = text.strip('\" ').strip()\n",
    "    name = \"\"\n",
    "    dimensions = \"\"\n",
    "    suffix = \"\"\n",
    "    \n",
    "    if '\\P' in text:\n",
    "        parts = text.split('\\P')\n",
    "        name = parts[0].strip()\n",
    "        dim_suffix = parts[1].strip()\n",
    "        if ' ' in dim_suffix:\n",
    "            dim_part, suffix_part = dim_suffix.rsplit(' ', 1)\n",
    "            dimensions = dim_part.strip()\n",
    "            suffix = suffix_part.strip()\n",
    "        else:\n",
    "            dimensions = dim_suffix.strip()\n",
    "    else:\n",
    "        parts = text.split(' ')\n",
    "        name = text  # Take the whole text as name\n",
    "        if len(parts) > 1 and parts[-1].isdigit():\n",
    "            # If last part is a digit, assume it's a numeric identifier\n",
    "            name = ' '.join(parts[:-1])\n",
    "            suffix = parts[-1]\n",
    "            \n",
    "    return name, dimensions, suffix\n",
    "\n",
    "def point_in_polygon(point, polygon):\n",
    "    \"\"\"\n",
    "    Check if a point is inside a polygon using ray casting algorithm.\n",
    "    \"\"\"\n",
    "    x, y = point\n",
    "    n = len(polygon)\n",
    "    inside = False\n",
    "    \n",
    "    p1x, p1y = polygon[0]\n",
    "    for i in range(1, n + 1):\n",
    "        p2x, p2y = polygon[i % n]\n",
    "        if y > min(p1y, p2y):\n",
    "            if y <= max(p1y, p2y):\n",
    "                if x <= max(p1x, p2x):\n",
    "                    if p1y != p2y:\n",
    "                        xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x\n",
    "                    if p1x == p2x or x <= xinters:\n",
    "                        inside = not inside\n",
    "        p1x, p1y = p2x, p2y\n",
    "    \n",
    "    return inside\n",
    "\n",
    "def is_dimension_text(text, is_on_dimension_layer=False):\n",
    "    \"\"\"\n",
    "    Determine if text is likely a dimension annotation.\n",
    "    \"\"\"\n",
    "    if is_on_dimension_layer:\n",
    "        return True\n",
    "        \n",
    "    text = text.strip()\n",
    "    \n",
    "    # Check if text contains dimension patterns with room name\n",
    "    if '\\P' in text:\n",
    "        parts = text.split('\\P')\n",
    "        if len(parts) >= 2:\n",
    "            dimension_part = parts[1].strip()\n",
    "            # Check if the dimension part contains dimension-like characters\n",
    "            return (\n",
    "                (\"'\" in dimension_part or '\"' in dimension_part) and \n",
    "                ('x' in dimension_part.lower() or 'X' in dimension_part)\n",
    "            )\n",
    "    \n",
    "    # Other dimension patterns\n",
    "    dimension_patterns = [\n",
    "        # Dimension notation with feet/inches\n",
    "        (\"'\" in text or '\"' in text) and ('x' in text.lower() or 'X' in text),\n",
    "        # Area notations\n",
    "        \"sq\" in text.lower() and any(c.isdigit() for c in text),\n",
    "        # Elevation markers\n",
    "        (text.startswith(\"EL\") or text.startswith(\"el\") or text.startswith(\"+\") or text.startswith(\"-\")) and any(c.isdigit() for c in text)\n",
    "    ]\n",
    "    \n",
    "    return any(dimension_patterns)\n",
    "\n",
    "def is_numeric_only(text):\n",
    "    \"\"\"\n",
    "    Check if text contains only numeric characters (possibly with some separators).\n",
    "    \"\"\"\n",
    "    # Remove common separators\n",
    "    cleaned_text = text.replace(',', '').replace('.', '').replace('-', '').replace(' ', '')\n",
    "    return cleaned_text.isdigit()\n",
    "\n",
    "def should_skip_numbering(text):\n",
    "    \"\"\"\n",
    "    Check if text should be skipped for numbering based on specific patterns.\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return True  # Skip empty text\n",
    "        \n",
    "    # Clean the text by removing format codes and trimming whitespace\n",
    "    clean_text = re.sub(r'\\\\pxi[^;]*;', '', text).strip()\n",
    "    text_upper = clean_text.upper()\n",
    "    \n",
    "    # Skip D1, D2, D3... W1, W2, W3... S1, S2, S3... patterns\n",
    "    pattern1 = re.compile(r'^[DWS]\\d+$')\n",
    "    \n",
    "    # Skip texts with parentheses containing +, -, or 00\n",
    "    pattern2 = re.compile(r'.*\\(\\+.*\\).*|.*\\(-.*\\).*|.*\\(00\\).*')\n",
    "    \n",
    "    # Skip specific text terms - all uppercase for consistent comparison\n",
    "    special_terms = [\n",
    "        \"-UP\", \"UP\", \"DN\", \"DOWN\", \n",
    "        \"PARKING AREA\", \"PARKING\", \"PARK\", \n",
    "        \"LIFT\", \"ELEVATOR\", \n",
    "        \"GARAGE\"\n",
    "    ]\n",
    "    \n",
    "    # Check if the text contains any of the special terms\n",
    "    contains_special_term = any(term in text_upper for term in special_terms)\n",
    "    \n",
    "    return (pattern1.match(clean_text) is not None or \n",
    "            pattern2.match(clean_text) is not None or \n",
    "            contains_special_term)\n",
    "\n",
    "\n",
    "\n",
    "def extract_text_by_floor_levels(dxf_path):\n",
    "    \"\"\"\n",
    "    Extract all text entities that are inside polylines, organized by floor levels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = ezdxf.readfile(dxf_path)\n",
    "        msp = doc.modelspace()\n",
    "        \n",
    "        # Floor level mapping\n",
    "        floor_mapping = {\n",
    "            #\"ground floor\": [\"ground floor\", \"ground\", \"g/f\", \"gf\", \"ground_floor\", \"0\", \"0 floor\"],\n",
    "            \"Floor 1\": [\"first floor\", \"first\", \"1/f\", \"ff\", \"f/f\", \"1f\", \"first_floor\", \"1\", \"1 floor\", \"1st floor\"],\n",
    "            \"Floor 2\": [\"second floor\", \"second\", \"2/f\", \"sf\", \"s/f\", \"2f\", \"second_floor\", \"2\", \"2 floor\", \"2nd floor\"],\n",
    "            \"Floor 3\": [\"third floor\", \"third\", \"3/f\", \"tf\", \"t/f\", \"3f\", \"third_floor\", \"3\", \"3 floor\", \"3rd floor\"],\n",
    "            \"Floor 4\": [\"fourth floor\", \"fourth\", \"4/f\", \"ff\", \"4f\", \"fourth_floor\", \"4\", \"4 floor\", \"4th floor\"],\n",
    "            \"Floor 5\": [\"fifth floor\", \"fifth\", \"5/f\", \"ff\", \"5f\", \"fifth_floor\", \"5\", \"5 floor\", \"5th floor\"],\n",
    "            \"Floor 6\": [\"sixth floor\", \"sixth\", \"6/f\", \"sf\", \"6f\", \"sixth_floor\", \"6\", \"6 floor\", \"6th floor\"],\n",
    "            \"Floor 7\": [\"seventh floor\", \"seventh\", \"7/f\", \"sf\", \"7f\", \"seventh_floor\", \"7\", \"7 floor\", \"7th floor\"],\n",
    "            \"Floor 8\": [\"eighth floor\", \"eighth\", \"8/f\", \"ef\", \"8f\", \"eighth_floor\", \"8\", \"8 floor\", \"8th floor\"],\n",
    "            \"terrace floor\": [\"terrace floor\", \"terrace\", \"t/f\", \"tf\", \"terrace_floor\", \"roof\", \"roof floor\"]\n",
    "        }\n",
    "        \n",
    "        # Get all layers in the document\n",
    "        layers = {layer.dxf.name for layer in doc.layers}\n",
    "        print(f\"Layers in the document: {', '.join(layers)}\")\n",
    "        \n",
    "        # Identify dimension layers\n",
    "        dimension_layers = set()\n",
    "        for layer in layers:\n",
    "            layer_lower = layer.lower()\n",
    "            if any(dim_keyword in layer_lower for dim_keyword in ['dim', 'dimension', 'dimensions', 'quote']):\n",
    "                dimension_layers.add(layer)\n",
    "                print(f\"Identified dimension layer: {layer}\")\n",
    "        \n",
    "        # Map layers to floor levels\n",
    "        floor_layers = {}\n",
    "        for layer in layers:\n",
    "            layer_lower = layer.lower()\n",
    "            for floor_name, patterns in floor_mapping.items():\n",
    "                if any(pattern.lower() in layer_lower for pattern in patterns):\n",
    "                    floor_layers[layer] = floor_name\n",
    "                    break\n",
    "        \n",
    "        if '0' in layers and '0' not in floor_layers:\n",
    "            floor_layers['0'] = \"default\"\n",
    "        \n",
    "        print(f\"Detected floor layers: {floor_layers}\")\n",
    "        \n",
    "        # Extract all closed polylines\n",
    "        all_polylines = []\n",
    "        polyline_id = 0\n",
    "        \n",
    "        for entity in msp.query('LWPOLYLINE POLYLINE'):\n",
    "            is_closed = False\n",
    "            vertices = []\n",
    "            \n",
    "            if entity.dxftype() == 'LWPOLYLINE':\n",
    "                is_closed = entity.closed\n",
    "                if is_closed:\n",
    "                    points = entity.get_points()\n",
    "                    vertices = [(p[0], p[1]) for p in points]\n",
    "            elif entity.dxftype() == 'POLYLINE':\n",
    "                is_closed = entity.is_closed\n",
    "                if is_closed:\n",
    "                    vertices = [(v.dxf.location[0], v.dxf.location[1]) for v in entity.vertices]\n",
    "            \n",
    "            if is_closed and len(vertices) >= 3:\n",
    "                layer = entity.dxf.layer\n",
    "                \n",
    "                floor = None\n",
    "                if layer in floor_layers:\n",
    "                    floor = floor_layers[layer]\n",
    "                else:\n",
    "                    floor = \"unknown\"\n",
    "                \n",
    "                # Calculate centroid and area\n",
    "                x_sum = sum(v[0] for v in vertices)\n",
    "                y_sum = sum(v[1] for v in vertices)\n",
    "                centroid = (x_sum / len(vertices), y_sum / len(vertices))\n",
    "                \n",
    "                area = 0.0\n",
    "                for i in range(len(vertices)):\n",
    "                    j = (i + 1) % len(vertices)\n",
    "                    area += vertices[i][0] * vertices[j][1]\n",
    "                    area -= vertices[j][0] * vertices[i][1]\n",
    "                area = abs(area) / 2.0\n",
    "                \n",
    "                all_polylines.append({\n",
    "                    'id': polyline_id,\n",
    "                    'entity': entity,\n",
    "                    'vertices': vertices,\n",
    "                    'layer': layer,\n",
    "                    'floor': floor,\n",
    "                    'centroid': centroid,\n",
    "                    'area': area,\n",
    "                    'contained_text': []\n",
    "                })\n",
    "                \n",
    "                polyline_id += 1\n",
    "        \n",
    "        print(f\"Found {len(all_polylines)} closed polylines\")\n",
    "        \n",
    "        # Extract all text entities\n",
    "        text_entities = []\n",
    "        \n",
    "        # Extract MTEXT entities\n",
    "        for entity in msp.query('MTEXT'):\n",
    "            layer = entity.dxf.layer\n",
    "            text = entity.text\n",
    "            position = entity.dxf.insert\n",
    "            \n",
    "            # Skip text entities on dimension layers\n",
    "            if layer in dimension_layers:\n",
    "                print(f\"Skipping MTEXT on dimension layer {layer}: '{text}'\")\n",
    "                continue\n",
    "            \n",
    "            floor = None\n",
    "            if layer in floor_layers:\n",
    "                floor = floor_layers[layer]\n",
    "            else:\n",
    "                text_lower = text.lower()\n",
    "                for floor_name, patterns in floor_mapping.items():\n",
    "                    if any(pattern.lower() in text_lower for pattern in patterns):\n",
    "                        floor = floor_name\n",
    "                        break\n",
    "            \n",
    "            if not floor:\n",
    "                floor = \"unknown\"\n",
    "            \n",
    "            text_entities.append({\n",
    "                'type': 'MTEXT',\n",
    "                'text': text,\n",
    "                'layer': layer,\n",
    "                'floor': floor,\n",
    "                'position': position,\n",
    "                'entity': entity,\n",
    "                'containing_polyline': None,\n",
    "                'is_dimension': layer in dimension_layers\n",
    "            })\n",
    "        \n",
    "        # Extract TEXT entities\n",
    "        for entity in msp.query('TEXT'):\n",
    "            layer = entity.dxf.layer\n",
    "            text = entity.dxf.text\n",
    "            position = entity.dxf.insert\n",
    "            \n",
    "            # Skip text entities on dimension layers\n",
    "            if layer in dimension_layers:\n",
    "                print(f\"Skipping TEXT on dimension layer {layer}: '{text}'\")\n",
    "                continue\n",
    "            \n",
    "            floor = None\n",
    "            if layer in floor_layers:\n",
    "                floor = floor_layers[layer]\n",
    "            else:\n",
    "                text_lower = text.lower()\n",
    "                for floor_name, patterns in floor_mapping.items():\n",
    "                    if any(pattern.lower() in text_lower for pattern in patterns):\n",
    "                        floor = floor_name\n",
    "                        break\n",
    "            \n",
    "            if not floor:\n",
    "                floor = \"unknown\"\n",
    "            \n",
    "            text_entities.append({\n",
    "                'type': 'TEXT',\n",
    "                'text': text,\n",
    "                'layer': layer,\n",
    "                'floor': floor,\n",
    "                'position': position,\n",
    "                'entity': entity,\n",
    "                'containing_polyline': None,\n",
    "                'is_dimension': layer in dimension_layers\n",
    "            })\n",
    "        \n",
    "        print(f\"Found {len(text_entities)} text entities\")\n",
    "        \n",
    "        # Determine which text entities are inside which polylines\n",
    "        for text_entity in text_entities:\n",
    "            position = text_entity['position']\n",
    "            point = (position[0], position[1])\n",
    "            \n",
    "            for poly in all_polylines:\n",
    "                if point_in_polygon(point, poly['vertices']):\n",
    "                    text_entity['containing_polyline'] = poly['id']\n",
    "                    poly['contained_text'].append(text_entity)\n",
    "                    \n",
    "                    # Update floor information\n",
    "                    if (poly['floor'] == \"default\" or poly['floor'] == \"unknown\") and text_entity['floor'] != \"unknown\":\n",
    "                        poly['floor'] = text_entity['floor']\n",
    "                    \n",
    "                    if text_entity['floor'] == \"unknown\" and poly['floor'] not in [\"default\", \"unknown\"]:\n",
    "                        text_entity['floor'] = poly['floor']\n",
    "                    \n",
    "                    break\n",
    "        \n",
    "        # Define floor order and prepare result structures\n",
    "        floor_order = [\"Floor 1\", \"Floor 2\", \"Floor 3\", \"Floor 4\", \"Floor 5\",\"Floor 6\",\"Floor 7\",\"Floor 8\", \"terrace floor\"]\n",
    "        text_by_floor = {floor: [] for floor in floor_order + [\"unknown\"]}\n",
    "        \n",
    "        polylines_with_text = [poly for poly in all_polylines if poly['contained_text']]\n",
    "        print(f\"Found {len(polylines_with_text)} polylines containing text\")\n",
    "        \n",
    "        # Organize text by floor\n",
    "        for floor in floor_order:\n",
    "            floor_polylines = [poly for poly in polylines_with_text if poly['floor'] == floor]\n",
    "            \n",
    "            for poly in floor_polylines:\n",
    "                for text in poly['contained_text']:\n",
    "                    # Parse text into components\n",
    "                    name, dimensions, suffix = slice_text_entity(text['text'])\n",
    "                    \n",
    "                    text_by_floor[floor].append({\n",
    "                        'original_text': text['text'],\n",
    "                        'name': name,\n",
    "                        'dimensions': dimensions,\n",
    "                        'suffix': suffix,\n",
    "                        'type': text['type'],\n",
    "                        'layer': text['layer'],\n",
    "                        'position': text['position'],\n",
    "                        'containing_polyline_id': poly['id'],\n",
    "                        'containing_polyline_layer': poly['layer'],\n",
    "                        'polyline_centroid': poly['centroid'],\n",
    "                        'is_dimension': text.get('is_dimension', False)\n",
    "                    })\n",
    "        \n",
    "        # Count texts by floor\n",
    "        text_counts = {floor: len(texts) for floor, texts in text_by_floor.items()}\n",
    "        \n",
    "        # Prepare results\n",
    "        results = {\n",
    "            'floor_order': floor_order,\n",
    "            'text_by_floor': text_by_floor,\n",
    "            'text_counts': text_counts,\n",
    "            'total_texts': sum(text_counts.values()),\n",
    "            'polylines_with_text': len(polylines_with_text),\n",
    "            'total_polylines': len(all_polylines),\n",
    "            'dimension_layers': dimension_layers\n",
    "        }\n",
    "        \n",
    "        # Organize text by polyline\n",
    "        text_by_polyline = {floor: {} for floor in floor_order + [\"unknown\"]}\n",
    "        \n",
    "        for floor in floor_order + [\"unknown\"]:\n",
    "            floor_polylines = [poly for poly in polylines_with_text if poly['floor'] == floor]\n",
    "            \n",
    "            for poly in floor_polylines:\n",
    "                poly_id = f\"poly_{poly['id']}\"\n",
    "                text_by_polyline[floor][poly_id] = []\n",
    "                \n",
    "                for text in poly['contained_text']:\n",
    "                    name, dimensions, suffix = slice_text_entity(text['text'])\n",
    "                    \n",
    "                    text_by_polyline[floor][poly_id].append({\n",
    "                        'original_text': text['text'],\n",
    "                        'name': name,\n",
    "                        'dimensions': dimensions,\n",
    "                        'suffix': suffix,\n",
    "                        'type': text['type'],\n",
    "                        'layer': text['layer'],\n",
    "                        'position': text['position'],\n",
    "                        'is_dimension': text.get('is_dimension', False)\n",
    "                    })\n",
    "        \n",
    "        results['text_by_polyline'] = text_by_polyline\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing DXF file: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def print_floor_text_summary(results):\n",
    "    \"\"\"\n",
    "    Print a summary of text entities by floor.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to display.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n===== TEXT ENTITIES BY FLOOR =====\")\n",
    "    print(f\"Total text entities found in polylines: {results['total_texts']}\")\n",
    "    print(f\"Total polylines with text: {results['polylines_with_text']} (out of {results['total_polylines']} total polylines)\")\n",
    "    print(f\"Dimension layers (skipped): {', '.join(results['dimension_layers'])}\")\n",
    "    \n",
    "    for floor in results['floor_order'] + [\"unknown\"]:\n",
    "        text_count = results['text_counts'][floor]\n",
    "        if text_count > 0:\n",
    "            print(f\"\\n--- {floor.upper()} ({text_count} text entities) ---\")\n",
    "            if floor in results['text_by_polyline']:\n",
    "                polyline_groups = results['text_by_polyline'][floor]\n",
    "                for poly_id, texts in polyline_groups.items():\n",
    "                    if texts:\n",
    "                        print(f\"\\n  Polyline {poly_id} ({len(texts)} texts):\")\n",
    "                        for text in texts:\n",
    "                            print(f\"    - Original: \\\"{text['original_text']}\\\"\")\n",
    "                            print(f\"    - Name: {text['name']}\")\n",
    "                            print(f\"    - Dimensions: {text['dimensions']}\")\n",
    "                            print(f\"    - Suffix: {text['suffix']} ({text['type']} on layer {text['layer']})\")\n",
    "                            if 'text' in text:\n",
    "                                print(f\"    - Numbered Text: \\\"{text['text']}\\\"\")\n",
    "                            if 'skipped_reason' in text:\n",
    "                                print(f\"    - Skipped: {text['skipped_reason']}\")\n",
    "\n",
    "def number_repeated_text_across_floors(results):\n",
    "    \"\"\"\n",
    "    Improved function to assign sequence numbers to repeated text entities across floors.\n",
    "    This version ensures all repeated text gets numbered, including dimensions.\n",
    "    With added constraints: Skip numbering for purely numeric values and specific patterns.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to process.\")\n",
    "        return None\n",
    "    \n",
    "    # Track room name occurrences globally across all floors\n",
    "    room_name_occurrences = {}\n",
    "    \n",
    "    # Count skipped and numbered text entities\n",
    "    skipped_count = 0\n",
    "    numbered_count = 0\n",
    "    numeric_skipped = 0\n",
    "    pattern_skipped = 0\n",
    "    \n",
    "    # Arrays for debugging\n",
    "    processed_texts = {\n",
    "        \"numbered\": [],\n",
    "        \"dimension_preserved\": [],\n",
    "        \"numeric_skipped\": [],\n",
    "        \"pattern_skipped\": []\n",
    "    }\n",
    "    \n",
    "    # Process each floor in order\n",
    "    floor_order = results['floor_order'] + [\"unknown\"]\n",
    "    \n",
    "    # First pass: Identify all unique room/space names and count occurrences\n",
    "    for floor in floor_order:\n",
    "        if floor in results['text_by_polyline']:\n",
    "            for poly_id, texts in results['text_by_polyline'][floor].items():\n",
    "                for text in texts:\n",
    "                    # Get the text content\n",
    "                    original_text = text['original_text'].strip()\n",
    "                    \n",
    "                    # Skip if text is just a number\n",
    "                    if is_numeric_only(original_text):\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip if text matches specific patterns\n",
    "                    if should_skip_numbering(original_text):\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract room name\n",
    "                    is_dimension = is_dimension_text(original_text, text.get('is_dimension', False))\n",
    "                    \n",
    "                    if is_dimension and '\\P' in original_text:\n",
    "                        # For text with dimensions, use the first part as room name\n",
    "                        room_name = original_text.split('\\P')[0].strip()\n",
    "                    else:\n",
    "                        # For regular text, use the whole text as room name\n",
    "                        # Remove any existing numbering if present\n",
    "                        parts = original_text.split()\n",
    "                        if len(parts) > 1 and parts[-1].isdigit():\n",
    "                            room_name = ' '.join(parts[:-1])\n",
    "                        else:\n",
    "                            room_name = original_text\n",
    "                    \n",
    "                    # Increment occurrence count for this room name\n",
    "                    if room_name in room_name_occurrences:\n",
    "                        room_name_occurrences[room_name] += 1\n",
    "                    else:\n",
    "                        room_name_occurrences[room_name] = 1\n",
    "    \n",
    "    # Filter to only keep names that appear more than once\n",
    "    repeated_room_names = {name: 0 for name, count in room_name_occurrences.items() if count > 1}\n",
    "    \n",
    "    # Second pass: Apply numbering to repeated text entities\n",
    "    for floor in floor_order:\n",
    "        if floor in results['text_by_polyline']:\n",
    "            for poly_id, texts in results['text_by_polyline'][floor].items():\n",
    "                if texts:\n",
    "                    # Sort texts by position (top to bottom, left to right)\n",
    "                    texts.sort(key=lambda t: (-t['position'][1], t['position'][0]))\n",
    "                    \n",
    "                    for text in texts:\n",
    "                        original_text = text['original_text'].strip()\n",
    "                        \n",
    "                        # Skip if text is just a number\n",
    "                        if is_numeric_only(original_text):\n",
    "                            text['text'] = original_text\n",
    "                            text['skipped_reason'] = \"Numeric value\"\n",
    "                            numeric_skipped += 1\n",
    "                            processed_texts[\"numeric_skipped\"].append(original_text)\n",
    "                            continue\n",
    "                        \n",
    "                        # Skip if text matches specific patterns (D1, W1, S1, etc. or (+...), (-...), (00), or special terms)\n",
    "                        if should_skip_numbering(original_text):\n",
    "                            text['text'] = original_text\n",
    "                            text['skipped_reason'] = \"Pattern match (D#, W#, S#, special terms, or parentheses patterns)\"\n",
    "                            pattern_skipped += 1\n",
    "                            processed_texts[\"pattern_skipped\"].append(original_text)\n",
    "                            continue\n",
    "                        \n",
    "                        is_dimension = is_dimension_text(original_text, text.get('is_dimension', False))\n",
    "                        \n",
    "                        if is_dimension and '\\P' in original_text:\n",
    "                            # Split into room name and dimensions\n",
    "                            room_part, dimension_part = original_text.split('\\P', 1)\n",
    "                            room_name = room_part.strip()\n",
    "                            \n",
    "                            # Only number if this is a repeated room name\n",
    "                            if room_name in repeated_room_names:\n",
    "                                repeated_room_names[room_name] += 1\n",
    "                                sequence_num = repeated_room_names[room_name]\n",
    "                                \n",
    "                                # Create numbered text with dimensions preserved\n",
    "                                numbered_text = f\"{room_name} {sequence_num}\\\\P{dimension_part}\"\n",
    "                                \n",
    "                                # Store the result\n",
    "                                text['name'] = f\"{room_name} {sequence_num}\"\n",
    "                                text['text'] = numbered_text\n",
    "                                text['sequence_number'] = sequence_num\n",
    "                                \n",
    "                                numbered_count += 1\n",
    "                                processed_texts[\"numbered\"].append({\n",
    "                                    \"original\": original_text,\n",
    "                                    \"numbered\": numbered_text\n",
    "                                })\n",
    "                            else:\n",
    "                                # Single occurrence, preserve original\n",
    "                                text['text'] = original_text\n",
    "                                processed_texts[\"dimension_preserved\"].append(original_text)\n",
    "                        else:\n",
    "                            # Regular text without dimensions\n",
    "                            # Remove any existing numbering\n",
    "                            parts = original_text.split()\n",
    "                            if len(parts) > 1 and parts[-1].isdigit():\n",
    "                                room_name = ' '.join(parts[:-1])\n",
    "                            else:\n",
    "                                room_name = original_text\n",
    "                            \n",
    "                            # Only number if this is a repeated room name\n",
    "                            if room_name in repeated_room_names:\n",
    "                                repeated_room_names[room_name] += 1\n",
    "                                sequence_num = repeated_room_names[room_name]\n",
    "                                \n",
    "                                # Create numbered text\n",
    "                                numbered_text = f\"{room_name} {sequence_num}\"\n",
    "                                \n",
    "                                # Store the result\n",
    "                                text['name'] = numbered_text\n",
    "                                text['text'] = numbered_text\n",
    "                                text['sequence_number'] = sequence_num\n",
    "                                \n",
    "                                numbered_count += 1\n",
    "                                processed_texts[\"numbered\"].append({\n",
    "                                    \"original\": original_text,\n",
    "                                    \"numbered\": numbered_text\n",
    "                                })\n",
    "                            else:\n",
    "                                # Single occurrence, preserve original\n",
    "                                text['text'] = original_text\n",
    "    \n",
    "    # Print results for debugging\n",
    "    print(\"\\nRoom types with numbering:\")\n",
    "    for room_name, count in room_name_occurrences.items():\n",
    "        if count > 1:\n",
    "            print(f\"{room_name}: {count} occurrences\")\n",
    "    \n",
    "    print(f\"\\nNumbered {numbered_count} text entities and preserved {len(processed_texts['dimension_preserved'])} unique text entities\")\n",
    "    print(f\"Skipped {numeric_skipped} numeric values\")\n",
    "    print(f\"Skipped {pattern_skipped} pattern-matched texts (D#, W#, S#, special terms, or parentheses patterns)\")\n",
    "    \n",
    "    # Print info about numbered texts for debugging\n",
    "    print(\"\\nNumbered texts (examples):\")\n",
    "    for numbered in processed_texts[\"numbered\"][:20]:  # Limit to 20 examples\n",
    "        print(f\"  - '{numbered['original']}'  '{numbered['numbered']}'\")\n",
    "    \n",
    "    if len(processed_texts[\"numbered\"]) > 20:\n",
    "        print(f\"  ... and {len(processed_texts['numbered']) - 20} more\")\n",
    "    \n",
    "    # Print info about skipped numeric values\n",
    "    print(\"\\nSkipped numeric values (examples):\")\n",
    "    for numeric in processed_texts[\"numeric_skipped\"][:20]:  # Limit to 20 examples\n",
    "        print(f\"  - '{numeric}'\")\n",
    "    \n",
    "    if len(processed_texts[\"numeric_skipped\"]) > 20:\n",
    "        print(f\"  ... and {len(processed_texts['numeric_skipped']) - 20} more\")\n",
    "    \n",
    "    # Print info about pattern-skipped values\n",
    "    print(\"\\nSkipped pattern-matched values (examples):\")\n",
    "    for pattern in processed_texts[\"pattern_skipped\"][:20]:  # Limit to 20 examples\n",
    "        print(f\"  - '{pattern}'\")\n",
    "    \n",
    "    if len(processed_texts[\"pattern_skipped\"]) > 20:\n",
    "        print(f\"  ... and {len(processed_texts['pattern_skipped']) - 20} more\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_numbered_text_to_dxf(input_dxf, output_dxf, results):\n",
    "    \"\"\"\n",
    "    Save the numbered text entities to a new DXF file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = ezdxf.readfile(input_dxf)\n",
    "        msp = doc.modelspace()\n",
    "        updated_count = 0\n",
    "        \n",
    "        # Map position to updated text\n",
    "        position_to_text = {}\n",
    "        \n",
    "        # Collect the updated text data\n",
    "        for floor in results['floor_order'] + [\"unknown\"]:\n",
    "            if floor in results['text_by_polyline']:\n",
    "                for poly_id, texts in results['text_by_polyline'][floor].items():\n",
    "                    for text in texts:\n",
    "                        pos_tuple = tuple(text['position'])\n",
    "                        if 'text' in text:  # Ensure 'text' key exists\n",
    "                            position_to_text[pos_tuple] = text['text']\n",
    "        \n",
    "        # Update TEXT entities\n",
    "        for entity in msp.query('TEXT'):\n",
    "            pos = tuple(entity.dxf.insert)\n",
    "            if pos in position_to_text:\n",
    "                entity.dxf.text = position_to_text[pos]\n",
    "                updated_count += 1\n",
    "        \n",
    "        # Update MTEXT entities\n",
    "        for entity in msp.query('MTEXT'):\n",
    "            pos = tuple(entity.dxf.insert)\n",
    "            if pos in position_to_text:\n",
    "                entity.text = position_to_text[pos]\n",
    "                updated_count += 1\n",
    "        \n",
    "        # Save the updated DXF file\n",
    "        doc.saveas(output_dxf)\n",
    "        print(f\"Updated {updated_count} text entities in the DXF file\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving numbered text to DXF: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def main6(input_dxf6, output_dxf, debug=True):\n",
    "    \"\"\"\n",
    "    Main function for processing DXF files - extracts text by floor levels,\n",
    "    numbers repeated text entities, and saves the result to a new DXF file.\n",
    "    \"\"\"\n",
    "    print(f\"Processing input file: {input_dxf6}\")\n",
    "    print(f\"Output will be saved to: {output_dxf}\")\n",
    "    \n",
    "    # Extract text entities from DXF file\n",
    "    results = extract_text_by_floor_levels(input_dxf6)\n",
    "    if not results:\n",
    "        print(\"Failed to extract text from DXF file.\")\n",
    "        return False\n",
    "    \n",
    "    # Number repeated text entities\n",
    "    results = number_repeated_text_across_floors(results)\n",
    "    if not results:\n",
    "        print(\"Failed to number text entities.\")\n",
    "        return False\n",
    "    \n",
    "    # Print summary if in debug mode\n",
    "    if debug:\n",
    "        print_floor_text_summary(results)\n",
    "    \n",
    "    # Save the updated DXF file\n",
    "    success = save_numbered_text_to_dxf(input_dxf6, output_dxf, results)\n",
    "    if success:\n",
    "        print(f\"Corrected DXF saved as {output_dxf}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to save corrected DXF.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948dd73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1287117c",
   "metadata": {},
   "source": [
    "# layer management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf726975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dxf(input_dxf7, output_dxf):\n",
    "    # 1. Takes input DXF\n",
    "    doc = ezdxf.readfile(input_dxf7)\n",
    "    msp = doc.modelspace()\n",
    "    \n",
    "    # 2. Deletes all layers excluding layer 0\n",
    "    layers_to_remove = [layer.dxf.name for layer in doc.layers if layer.dxf.name != '0']\n",
    "    for layer_name in layers_to_remove:\n",
    "        doc.layers.remove(layer_name)\n",
    "    \n",
    "    # 3. Detects only text or mtext and 4. Creates layers from detected text/mtext\n",
    "    for entity in msp:\n",
    "        if entity.dxftype() in ['TEXT', 'MTEXT']:\n",
    "            text_content = entity.dxf.text if entity.dxftype() == 'TEXT' else entity.dxf.text\n",
    "            # Replace non-alphanumeric (except underscore) with underscore, max 31 chars\n",
    "            layer_name = ''.join('' if not (c.isalnum() or c == '') else c for c in text_content)[:31]\n",
    "            # If empty after processing, use a default name\n",
    "            if not layer_name:\n",
    "                layer_name = \"TEXT_LAYER\"\n",
    "            if layer_name not in doc.layers:\n",
    "                doc.layers.new(layer_name)\n",
    "            entity.dxf.layer = layer_name\n",
    "    \n",
    "    # 5. Gives out final DXF file\n",
    "    doc.saveas(output_dxf)\n",
    "    \n",
    "def main7(input_dxf7, output_dxf):\n",
    "    process_dxf(input_dxf7, output_dxf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34953d3-56ad-4964-91eb-84221ddd5261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_final(input_dxf0, final_output,tolerance, max_iterations):\n",
    "    \"\"\"\n",
    "    Main function to handle sequential DXF processing and clean up intermediate files.\n",
    "\n",
    "    Args:\n",
    "        input_dxf1 (str): The initial input DXF file.\n",
    "        final_output (str): Final output DXF file name.\n",
    "    \"\"\"\n",
    "    intermediate_files = [\"1.dxf\", \"2.dxf\", \"3.dxf\", \"4.dxf\",\"5.dxf\",\"6.dxf\",\"7.dxf\"]\n",
    "\n",
    "    # Sequential processing using provided main functions\n",
    "    main0(input_dxf0=input_dxf0, output_dxf=intermediate_files[0],tolerance=tolerance,max_iterations=max_iterations)\n",
    "    main1(input_dxf1=intermediate_files[0], output_dxf=intermediate_files[1])\n",
    "    main2(input_dxf2=intermediate_files[1], output_dxf=intermediate_files[2])\n",
    "    main3(input_dxf3=intermediate_files[2], output_dxf=intermediate_files[3])\n",
    "    main4(input_dxf4=intermediate_files[3], output_dxf=intermediate_files[4])\n",
    "    main5(input_dxf5=intermediate_files[4], output_dxf=intermediate_files[5])\n",
    "    main6(input_dxf6=intermediate_files[5], output_dxf=intermediate_files[6])\n",
    "    main7(input_dxf7=intermediate_files[6], output_dxf=final_output)\n",
    "    files_structure = {\"Intermediate Files\": intermediate_files}\n",
    "    print(\"Files structure:\", files_structure)\n",
    "\n",
    "    for file in intermediate_files:\n",
    "        if os.path.exists(file):\n",
    "            try:\n",
    "                os.remove(file)\n",
    "                print(f\"Deleted: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting {file}: {e}\")\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2351f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OVERKILL process on 4new4.dxf\n",
      "Using tolerance value: 1e-06\n",
      "Maximum iterations: 5\n",
      "This may take a moment for complex drawings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Identifying polylines in Floor layers...\n",
      "INFO: Found 0 polylines in Floor layers\n",
      "INFO: Starting OVERKILL process...\n",
      "INFO: \n",
      "--- Iteration 1 ---\n",
      "INFO: Processing 707 entities (707 lines, 0 arcs)...\n",
      "INFO: Removed 53 duplicate entities.\n",
      "INFO: Analyzing overlapping and connecting entities...\n",
      "INFO: Building spatial index...\n",
      "INFO: Building mergeable entity graph...\n",
      "INFO: Found 55 sets of entities that can be merged.\n",
      "INFO: Merging entity components...\n",
      "INFO: Iteration 1 results:\n",
      "INFO:   - Entities merged/deleted: 151\n",
      "INFO:   - New entities created: 55\n",
      "INFO: \n",
      "--- Iteration 2 ---\n",
      "INFO: Processing 558 entities (558 lines, 0 arcs)...\n",
      "INFO: Analyzing overlapping and connecting entities...\n",
      "INFO: Building spatial index...\n",
      "INFO: Building mergeable entity graph...\n",
      "INFO: Found 0 sets of entities that can be merged.\n",
      "INFO: No more entities can be merged.\n",
      "INFO: \n",
      "OVERKILL operation completed in 2.94 seconds:\n",
      "INFO:   - Iterations performed: 2\n",
      "INFO:   - Original entity count: 707\n",
      "INFO:   - Total entities deleted: 204\n",
      "INFO:   - Total new entities created: 55\n",
      "INFO:   - Final entity count: 558\n",
      "INFO:   - Entity reduction: 149 entities (21.1%)\n",
      "INFO:   - Result saved to 1.dxf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 96 polylines to 256 lines\n",
      "Skipped 3 polylines on floor layers\n",
      "Remaining polylines after conversion: 3\n",
      "DXF file saved as 3.dxf\n",
      "Loading DXF file: 3.dxf\n",
      "\n",
      "Block Counts:\n",
      "Corrected TEXT: 'Ground floor plan'  'GROUND FLOOR PLAN'\n",
      "Corrected TEXT: 'First floor plan'  'FIRST FLOOR PLAN'\n",
      "Corrected TEXT: 'Terrace plan'  'TERRACE PLAN'\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Living room\\P17' X 14''  'LIVING ROOM\\P17' X 14''\n",
      "Corrected MTEXT: 'Mandir + Dining area\\P22'3\" X 10''  'MANDIR + DINING AREA\\P22'3\" X 10''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Dressing area\\P5'3\" X 7''  'DRESSING AREA\\P5'3\" X 7''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Bedroom 2\\P14'3\" X 12''  'BEDROOM\\P14'3\" X 12''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Toilet\\P5' X 7''  'BATHROOM\\P5' X 7''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Dressing area\\P5' X 6'7\"'  'DRESSING AREA\\P5' X 6'7\"'\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Bedroom 1\\P10' X 12''  'BEDROOM\\P10' X 12''\n",
      "Corrected MTEXT: '3' Wide O.T.S\\Ppond / garden\\Parea'  'O.T.S\\Ppond / garden\\Parea'\n",
      "Corrected MTEXT: 'Storage + Wash area\\P11'  X 5''  'STORAGE + WASH AREA\\P11'  X 5''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Parking area\\P13' X 13'7\"'  'PARKING AREA\\P13' X 13'7\"'\n",
      "Corrected MTEXT: '3' wide area'  '3' WIDE AREA'\n",
      "Corrected MTEXT: 'Kitchen\\P11'  X 14''  'KITCHEN\\P11'  X 14''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Toilet\\P5'3\" X 7''  'BATHROOM\\P5'3\" X 7''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Outdoor sitting\\P7'9\" X 5'6\"'  'OUTDOOR SITTING\\P7'9\" X 5'6\"'\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Balcony\\P4' X 5''  'BALCONY\\P4' X 5''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Living room\\P17' X 14''  'LIVING ROOM\\P17' X 14''\n",
      "Corrected MTEXT: 'Mandir + Dining area\\P22'3\" X 10''  'MANDIR + DINING AREA\\P22'3\" X 10''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Dressing area\\P5'3\" X 7''  'DRESSING AREA\\P5'3\" X 7''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Bedroom 4\\P14'3\" X 12''  'BEDROOM\\P14'3\" X 12''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Toilet\\P5' X 7''  'BATHROOM\\P5' X 7''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Dressing area\\P5' X 6'7\"'  'DRESSING AREA\\P5' X 6'7\"'\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Bedroom 3\\P10' X 12''  'BEDROOM\\P10' X 12''\n",
      "Corrected MTEXT: '3' Wide O.T.S\\Ppond / garden\\Parea'  'O.T.S\\Ppond / garden\\Parea'\n",
      "Corrected MTEXT: 'Storage + Wash area\\P11'  X 5''  'STORAGE + WASH AREA\\P11'  X 5''\n",
      "Corrected MTEXT: 'Kitchen\\P11'  X 14''  'KITCHEN\\P11'  X 14''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Toilet\\P5'3\" X 7''  'BATHROOM\\P5'3\" X 7''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Balcony\\P5' X 4''  'BALCONY\\P5' X 4''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Balcony\\P5' X 4''  'BALCONY\\P5' X 4''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Parking area\\P13' X 13'7\"'  'PARKING AREA\\P13' X 13'7\"'\n",
      "Corrected MTEXT: '3' wide area'  '3' WIDE AREA'\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Balcony\\P4' X 5''  'BALCONY\\P4' X 5''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Balcony\\P4' X 5''  'BALCONY\\P4' X 5''\n",
      "Corrected MTEXT: '\\pxi-3,l4,t4;Balcony\\P4' X 5''  'BALCONY\\P4' X 5''\n",
      "Processing input file: 6.dxf\n",
      "Output will be saved to: 7.dxf\n",
      "Layers in the document: TEXT, STAIRS, wall, Floor3, Floor1, FURNITURE, K_THUOC, Defpoints, Floor2, 0\n",
      "Detected floor layers: {'Floor3': 'Floor 3', 'Floor1': 'Floor 1', 'Defpoints': 'Floor 8', 'Floor2': 'Floor 2', '0': 'default'}\n",
      "Found 3 closed polylines\n",
      "Found 36 text entities\n",
      "Found 2 polylines containing text\n",
      "\n",
      "Room types with numbering:\n",
      "LIVING ROOM: 2 occurrences\n",
      "MANDIR + DINING AREA: 2 occurrences\n",
      "DRESSING AREA: 4 occurrences\n",
      "BEDROOM: 4 occurrences\n",
      "BATHROOM: 4 occurrences\n",
      "O.T.S\\Ppond / garden\\Parea: 2 occurrences\n",
      "STORAGE + WASH AREA: 2 occurrences\n",
      "3' WIDE AREA: 2 occurrences\n",
      "KITCHEN: 2 occurrences\n",
      "\n",
      "Numbered 24 text entities and preserved 2 unique text entities\n",
      "Skipped 0 numeric values\n",
      "Skipped 2 pattern-matched texts (D#, W#, S#, special terms, or parentheses patterns)\n",
      "\n",
      "Numbered texts (examples):\n",
      "  - 'STORAGE + WASH AREA\\P11'  X 5''  'STORAGE + WASH AREA 1\\P11'  X 5''\n",
      "  - 'BEDROOM\\P14'3\" X 12''  'BEDROOM 1\\P14'3\" X 12''\n",
      "  - 'KITCHEN\\P11'  X 14''  'KITCHEN 1\\P11'  X 14''\n",
      "  - 'BATHROOM\\P5'3\" X 7''  'BATHROOM 1\\P5'3\" X 7''\n",
      "  - 'DRESSING AREA\\P5'3\" X 7''  'DRESSING AREA 1\\P5'3\" X 7''\n",
      "  - 'MANDIR + DINING AREA\\P22'3\" X 10''  'MANDIR + DINING AREA 1\\P22'3\" X 10''\n",
      "  - 'O.T.S\\Ppond / garden\\Parea'  'O.T.S\\Ppond / garden\\Parea 1'\n",
      "  - 'BATHROOM\\P5' X 7''  'BATHROOM 2\\P5' X 7''\n",
      "  - 'LIVING ROOM\\P17' X 14''  'LIVING ROOM 1\\P17' X 14''\n",
      "  - 'DRESSING AREA\\P5' X 6'7\"'  'DRESSING AREA 2\\P5' X 6'7\"'\n",
      "  - 'BEDROOM\\P10' X 12''  'BEDROOM 2\\P10' X 12''\n",
      "  - '3' WIDE AREA'  '3' WIDE AREA 1'\n",
      "  - 'STORAGE + WASH AREA\\P11'  X 5''  'STORAGE + WASH AREA 2\\P11'  X 5''\n",
      "  - 'BEDROOM\\P14'3\" X 12''  'BEDROOM 3\\P14'3\" X 12''\n",
      "  - 'KITCHEN\\P11'  X 14''  'KITCHEN 2\\P11'  X 14''\n",
      "  - 'BATHROOM\\P5'3\" X 7''  'BATHROOM 3\\P5'3\" X 7''\n",
      "  - 'DRESSING AREA\\P5'3\" X 7''  'DRESSING AREA 3\\P5'3\" X 7''\n",
      "  - 'MANDIR + DINING AREA\\P22'3\" X 10''  'MANDIR + DINING AREA 2\\P22'3\" X 10''\n",
      "  - 'O.T.S\\Ppond / garden\\Parea'  'O.T.S\\Ppond / garden\\Parea 2'\n",
      "  - 'BATHROOM\\P5' X 7''  'BATHROOM 4\\P5' X 7''\n",
      "  ... and 4 more\n",
      "\n",
      "Skipped numeric values (examples):\n",
      "\n",
      "Skipped pattern-matched values (examples):\n",
      "  - 'PARKING AREA\\P13' X 13'7\"'\n",
      "  - 'PARKING AREA\\P13' X 13'7\"'\n",
      "\n",
      "===== TEXT ENTITIES BY FLOOR =====\n",
      "Total text entities found in polylines: 28\n",
      "Total polylines with text: 2 (out of 3 total polylines)\n",
      "Dimension layers (skipped): \n",
      "\n",
      "--- FLOOR 1 (14 text entities) ---\n",
      "\n",
      "  Polyline poly_0 (14 texts):\n",
      "    - Original: \"STORAGE + WASH AREA\\P11'  X 5'\"\n",
      "    - Name: STORAGE + WASH AREA 1\n",
      "    - Dimensions: 11'  X\n",
      "    - Suffix: 5' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"STORAGE + WASH AREA 1\\P11'  X 5'\"\n",
      "    - Original: \"BEDROOM\\P14'3\" X 12'\"\n",
      "    - Name: BEDROOM 1\n",
      "    - Dimensions: 14'3\" X\n",
      "    - Suffix: 12' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"BEDROOM 1\\P14'3\" X 12'\"\n",
      "    - Original: \"KITCHEN\\P11'  X 14'\"\n",
      "    - Name: KITCHEN 1\n",
      "    - Dimensions: 11'  X\n",
      "    - Suffix: 14' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"KITCHEN 1\\P11'  X 14'\"\n",
      "    - Original: \"BATHROOM\\P5'3\" X 7'\"\n",
      "    - Name: BATHROOM 1\n",
      "    - Dimensions: 5'3\" X\n",
      "    - Suffix: 7' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"BATHROOM 1\\P5'3\" X 7'\"\n",
      "    - Original: \"DRESSING AREA\\P5'3\" X 7'\"\n",
      "    - Name: DRESSING AREA 1\n",
      "    - Dimensions: 5'3\" X\n",
      "    - Suffix: 7' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"DRESSING AREA 1\\P5'3\" X 7'\"\n",
      "    - Original: \"MANDIR + DINING AREA\\P22'3\" X 10'\"\n",
      "    - Name: MANDIR + DINING AREA 1\n",
      "    - Dimensions: 22'3\" X\n",
      "    - Suffix: 10' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"MANDIR + DINING AREA 1\\P22'3\" X 10'\"\n",
      "    - Original: \"O.T.S\\Ppond / garden\\Parea\"\n",
      "    - Name: O.T.S\\Ppond / garden\\Parea 1\n",
      "    - Dimensions: pond /\n",
      "    - Suffix: garden (MTEXT on layer 0)\n",
      "    - Numbered Text: \"O.T.S\\Ppond / garden\\Parea 1\"\n",
      "    - Original: \"BATHROOM\\P5' X 7'\"\n",
      "    - Name: BATHROOM 2\n",
      "    - Dimensions: 5' X\n",
      "    - Suffix: 7' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"BATHROOM 2\\P5' X 7'\"\n",
      "    - Original: \"LIVING ROOM\\P17' X 14'\"\n",
      "    - Name: LIVING ROOM 1\n",
      "    - Dimensions: 17' X\n",
      "    - Suffix: 14' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"LIVING ROOM 1\\P17' X 14'\"\n",
      "    - Original: \"DRESSING AREA\\P5' X 6'7\"\"\n",
      "    - Name: DRESSING AREA 2\n",
      "    - Dimensions: 5' X\n",
      "    - Suffix: 6'7 (MTEXT on layer 0)\n",
      "    - Numbered Text: \"DRESSING AREA 2\\P5' X 6'7\"\"\n",
      "    - Original: \"OUTDOOR SITTING\\P7'9\" X 5'6\"\"\n",
      "    - Name: OUTDOOR SITTING\n",
      "    - Dimensions: 7'9\" X\n",
      "    - Suffix: 5'6 (MTEXT on layer 0)\n",
      "    - Numbered Text: \"OUTDOOR SITTING\\P7'9\" X 5'6\"\"\n",
      "    - Original: \"BEDROOM\\P10' X 12'\"\n",
      "    - Name: BEDROOM 2\n",
      "    - Dimensions: 10' X\n",
      "    - Suffix: 12' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"BEDROOM 2\\P10' X 12'\"\n",
      "    - Original: \"PARKING AREA\\P13' X 13'7\"\"\n",
      "    - Name: PARKING AREA\n",
      "    - Dimensions: 13' X\n",
      "    - Suffix: 13'7 (MTEXT on layer 0)\n",
      "    - Numbered Text: \"PARKING AREA\\P13' X 13'7\"\"\n",
      "    - Skipped: Pattern match (D#, W#, S#, special terms, or parentheses patterns)\n",
      "    - Original: \"3' WIDE AREA\"\n",
      "    - Name: 3' WIDE AREA 1\n",
      "    - Dimensions: \n",
      "    - Suffix:  (MTEXT on layer 0)\n",
      "    - Numbered Text: \"3' WIDE AREA 1\"\n",
      "\n",
      "--- FLOOR 2 (14 text entities) ---\n",
      "\n",
      "  Polyline poly_1 (14 texts):\n",
      "    - Original: \"STORAGE + WASH AREA\\P11'  X 5'\"\n",
      "    - Name: STORAGE + WASH AREA 2\n",
      "    - Dimensions: 11'  X\n",
      "    - Suffix: 5' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"STORAGE + WASH AREA 2\\P11'  X 5'\"\n",
      "    - Original: \"BEDROOM\\P14'3\" X 12'\"\n",
      "    - Name: BEDROOM 3\n",
      "    - Dimensions: 14'3\" X\n",
      "    - Suffix: 12' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"BEDROOM 3\\P14'3\" X 12'\"\n",
      "    - Original: \"KITCHEN\\P11'  X 14'\"\n",
      "    - Name: KITCHEN 2\n",
      "    - Dimensions: 11'  X\n",
      "    - Suffix: 14' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"KITCHEN 2\\P11'  X 14'\"\n",
      "    - Original: \"BATHROOM\\P5'3\" X 7'\"\n",
      "    - Name: BATHROOM 3\n",
      "    - Dimensions: 5'3\" X\n",
      "    - Suffix: 7' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"BATHROOM 3\\P5'3\" X 7'\"\n",
      "    - Original: \"DRESSING AREA\\P5'3\" X 7'\"\n",
      "    - Name: DRESSING AREA 3\n",
      "    - Dimensions: 5'3\" X\n",
      "    - Suffix: 7' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"DRESSING AREA 3\\P5'3\" X 7'\"\n",
      "    - Original: \"MANDIR + DINING AREA\\P22'3\" X 10'\"\n",
      "    - Name: MANDIR + DINING AREA 2\n",
      "    - Dimensions: 22'3\" X\n",
      "    - Suffix: 10' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"MANDIR + DINING AREA 2\\P22'3\" X 10'\"\n",
      "    - Original: \"O.T.S\\Ppond / garden\\Parea\"\n",
      "    - Name: O.T.S\\Ppond / garden\\Parea 2\n",
      "    - Dimensions: pond /\n",
      "    - Suffix: garden (MTEXT on layer 0)\n",
      "    - Numbered Text: \"O.T.S\\Ppond / garden\\Parea 2\"\n",
      "    - Original: \"BATHROOM\\P5' X 7'\"\n",
      "    - Name: BATHROOM 4\n",
      "    - Dimensions: 5' X\n",
      "    - Suffix: 7' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"BATHROOM 4\\P5' X 7'\"\n",
      "    - Original: \"LIVING ROOM\\P17' X 14'\"\n",
      "    - Name: LIVING ROOM 2\n",
      "    - Dimensions: 17' X\n",
      "    - Suffix: 14' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"LIVING ROOM 2\\P17' X 14'\"\n",
      "    - Original: \"DRESSING AREA\\P5' X 6'7\"\"\n",
      "    - Name: DRESSING AREA 4\n",
      "    - Dimensions: 5' X\n",
      "    - Suffix: 6'7 (MTEXT on layer 0)\n",
      "    - Numbered Text: \"DRESSING AREA 4\\P5' X 6'7\"\"\n",
      "    - Original: \"BEDROOM\\P10' X 12'\"\n",
      "    - Name: BEDROOM 4\n",
      "    - Dimensions: 10' X\n",
      "    - Suffix: 12' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"BEDROOM 4\\P10' X 12'\"\n",
      "    - Original: \"BALCONY\\P4' X 5'\"\n",
      "    - Name: BALCONY\n",
      "    - Dimensions: 4' X\n",
      "    - Suffix: 5' (MTEXT on layer 0)\n",
      "    - Numbered Text: \"BALCONY\\P4' X 5'\"\n",
      "    - Original: \"PARKING AREA\\P13' X 13'7\"\"\n",
      "    - Name: PARKING AREA\n",
      "    - Dimensions: 13' X\n",
      "    - Suffix: 13'7 (MTEXT on layer 0)\n",
      "    - Numbered Text: \"PARKING AREA\\P13' X 13'7\"\"\n",
      "    - Skipped: Pattern match (D#, W#, S#, special terms, or parentheses patterns)\n",
      "    - Original: \"3' WIDE AREA\"\n",
      "    - Name: 3' WIDE AREA 2\n",
      "    - Dimensions: \n",
      "    - Suffix:  (MTEXT on layer 0)\n",
      "    - Numbered Text: \"3' WIDE AREA 2\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 28 text entities in the DXF file\n",
      "Corrected DXF saved as 7.dxf\n",
      "Files structure: {'Intermediate Files': ['1.dxf', '2.dxf', '3.dxf', '4.dxf', '5.dxf', '6.dxf', '7.dxf']}\n",
      "Deleted: 1.dxf\n",
      "Deleted: 2.dxf\n",
      "Deleted: 3.dxf\n",
      "Deleted: 4.dxf\n",
      "Deleted: 5.dxf\n",
      "Deleted: 6.dxf\n",
      "Deleted: 7.dxf\n"
     ]
    }
   ],
   "source": [
    "main_final(\n",
    "    input_dxf0=\"4new4.dxf\",\n",
    "    final_output=\"T11modified.dxf\",\n",
    "    tolerance = 0.000001,\n",
    "    max_iterations = 5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d722a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
